<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Alessandro Puccetti</title>
    <link>https://alepuccetti.github.io/post/</link>
      <atom:link href="https://alepuccetti.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 24 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://alepuccetti.github.io/images/icon_hu506a0a16710220c3fddbb4e47b0663c6_32981_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://alepuccetti.github.io/post/</link>
    </image>
    
    <item>
      <title>Saving Bank with Cloud Workflows</title>
      <link>https://alepuccetti.github.io/post/saving-bank-with-cloud-workflows/</link>
      <pubDate>Thu, 24 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/saving-bank-with-cloud-workflows/</guid>
      <description>&lt;p&gt;“Workflows is a fully-managed orchestration platform that executes services in an order that you define”.&lt;/p&gt;


















&lt;figure id=&#34;figure-workflow&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./workflow_wood.jpeg&#34; data-caption=&#34;Workflow&#34;&gt;


  &lt;img src=&#34;./workflow_wood.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Workflow
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In this blogpost, I will show you how I was able to increase performance, reduce cost, and improve code quality by migrating our ETL orchestration from Cloud Composer to Cloud workflows.&lt;/p&gt;
&lt;h2 id=&#34;the-journey&#34;&gt;The Journey&lt;/h2&gt;
&lt;p&gt;I am a die-hard fan of managed and serverless services.
That’s because they allow me to spend more time on developing interesting projects instead of maintaining infrastructure (boring!).&lt;/p&gt;
&lt;p&gt;Few years ago, at Huq Industries, we were redesigning our infrastructure by using BigQuery and Dataflow for our analytics and batch processing.
During this process, we also needed to strengthen the orchestration and scheduling of our ETLs and data delivery processes.
After evaluating different solutions, we decided to implement our orchestration with Cloud Composer (fully managed Apache Airflow solution from Google Cloud).
I knew from the beginning that Airflow would have been an overkill for our needs.
In particular because we don’t process data locally on the Airflow cluster but only via external services like Dataflow and BigQuery.
However, I wanted to have the peace of mind of a fully-managed solution and an easy-to-use system.&lt;/p&gt;
&lt;p&gt;Although Cloud Composer has been great for day-to-day operation, it also caused some pain along the way, especially when I needed to re-run data pipelines spanning across a few years (see my previous blog post regarding Airflow short-living tasks).
Lastly, having a cluster always up and running when you use it only a few hours a day is quite the waste of resources, but it was necessary for the business operations.&lt;/p&gt;
&lt;h2 id=&#34;the-dawn-of-serverless-orchestration&#34;&gt;The Dawn of Serverless Orchestration&lt;/h2&gt;
&lt;p&gt;When Google announced Cloud Workflows, I was very excited because I saw from the start that it would have been perfect for us.
So, I decided to take Cloud Workflows out for a spin.&lt;/p&gt;
&lt;p&gt;I started migrating the data pipelines and data export, and, to my pleasant surprise, the migration was incredibly fast and the final result was a much &lt;strong&gt;better code&lt;/strong&gt; base.
One could  describe it as “directly to the point”.
This new implementation drastically improved code readability, maintainability, and reusability.
The second benefit was the ludicrous increase in end-to-end performance.
In fact, before Cloud Workflows, a daily task needed about 30 minutes to complete, now it needs only 4 minutes (~8X faster).&lt;/p&gt;
&lt;p&gt;This speed improvement is even greater when running full history exports, while before it took almost a month, now it takes less than a working day (&lt;strong&gt;~40X&lt;/strong&gt;).
I want to highlight that I did not do any optimization to the external ETL jobs, so the performance improvements can all be attributed to Cloud Workflows.&lt;/p&gt;
&lt;p&gt;The main reason for this incredible improvement is that the external tasks are quick compared to the scheduling and instantiating time of Airflow tasks.
Cloud Workflows &lt;strong&gt;remove completely this non-processing time overhead&lt;/strong&gt;, so now the pipelines only last the time needed by the external service to complete.
A little drawback is that Cloud Workflows does not provide scheduling as a native feature like Airflow. On the other hand, it is incredibly simple to schedule your Workflows using Cloud Scheduler.&lt;/p&gt;
&lt;p&gt;The last, but definitely not least, advantage is the cost saving.
To be able to meet the clients&amp;rsquo; needs, we had to keep the Airflow cluster running 24/7 (even if tasks had about 3 hours of runtime per day) costing about 900$/month.
With Cloud workflows, per step pricing model you just pay for how many “actions” you execute.
For this reason, the &lt;strong&gt;cost dropped by several orders of magnitude&lt;/strong&gt;. Running the daily tasks with Cloud workflows costs us 0.05$ per day (1.5$/month) which is a cost reduction of &lt;strong&gt;600X&lt;/strong&gt;.
This makes the new system almost running for free.
The cost saving is staggering even for the full history re-running example, which boils down to just 70$.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, Cloud Workflows is the perfect match. It allows you to create simple and clear orchestration logic with amazing performance improvement and cost reduction.
At the moment of writing, Cloud Workflows is still a fairly new product in active development but it’s improving at a very high pace.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slots to the (Wallet) Rescue</title>
      <link>https://alepuccetti.github.io/post/slots-to-the-wallet-rescue/</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/slots-to-the-wallet-rescue/</guid>
      <description>

















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./lower_costs.jpeg&#34; &gt;


  &lt;img src=&#34;./lower_costs.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;At &lt;a href=&#34;https://huq.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huq Industries&lt;/a&gt;, we collect and process 30 billions geo-events a month from more than 8 million devices.
At the time of writing we collected and enriched more than 300 billions events all stored in BigQuery table for a total of about 85 TB.&lt;/p&gt;
&lt;p&gt;In this blog post, I will demonstrate how we reduced the cost a single expensive job from more than 425$ using on-demand pricing (5$/TB) to just 25$ using flat-rate pricing.&lt;/p&gt;
&lt;p&gt;Usually, flat-rate pricing is generally used when you want predictable costs and have a stable and continue slots usage.
However, flat-rate pricing can be leveraged also to reduce the cost of even a single job alone.&lt;/p&gt;
&lt;p&gt;In this post, I will use a real-world scenario from one of our clients to show how flat-rate pricing helped us saving lots of money when performing this very data-intensive job.&lt;/p&gt;
&lt;p&gt;For a more detailed explanation of &lt;strong&gt;what&lt;/strong&gt; are and &lt;strong&gt;how&lt;/strong&gt; BigQuery slots work refer to the &lt;a href=&#34;https://cloud.google.com/bigquery/docs/slots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official docs&lt;/a&gt;.
To put it simply, you can think to a BigQuery slot as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A virtual CPU used by BigQuery to execute SQL queries.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;bigquery-slots-commitment-and-reservation&#34;&gt;BigQuery Slots Commitment and Reservation&lt;/h2&gt;
&lt;p&gt;Before we start using BigQuery flat-rate, we need to have a quick introduction on how to &lt;strong&gt;provision&lt;/strong&gt; and &lt;strong&gt;allocate&lt;/strong&gt; slots reservations.
If you already have familiarity with this slots commitment and reservation feel free to skip this section and jump directly to the next part.&lt;/p&gt;
&lt;p&gt;We will follow the Google &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reservations-get-started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;best practice&lt;/a&gt; to set up the environment to use slots reservations.
First, we create a new GCP project to only manage slots and reservation.
Then, using the &amp;ldquo;Reservation&amp;rdquo; menu from te BigQuery UI, we buy a &lt;strong&gt;Flex&lt;/strong&gt; slots commitment.
This type of commitment is more expensive than monthly or annual ones, but it can be cancelled any time after 60 seconds.
Tip: Use the right-hand side widget to get an estimation of hourly/monthly cost of your commitment.&lt;/p&gt;
&lt;p&gt;Now that we have available slots, we need to create a &lt;strong&gt;reservation&lt;/strong&gt;.
You can grant to a reservation as many slots you want (up to the number you bought) and you can create as many reservations you like.
Slots not assigned to any reservation go in the default one. In case they are idle, they can be used by any reservation.&lt;/p&gt;
&lt;p&gt;The last step is to create an &lt;strong&gt;assignment&lt;/strong&gt; by assigning a reservation to one of our project/folder.
Now we are ready to run our job using flat-rate pricing instead of on-demand.&lt;/p&gt;
&lt;h2 id=&#34;the-scenario&#34;&gt;The Scenario&lt;/h2&gt;
&lt;p&gt;One of our client wants to run analytics on our entire events history,
however, they have some specific matching criteria for qualifying an event.
Therefore, the actual data they need is much smaller than our full history table.
Due to the custom nature of the filtering logic, we had no other option than running a query to select all the qualify events.
Below, you can find a mock query that could be used to generate the client dataset.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/607e91e54a27b7bdb1a9ef3c58cccaa1.js?file=example.sql&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;start-small&#34;&gt;Start Small&lt;/h2&gt;
&lt;p&gt;For simplicity, I will calculate the cost by multiplying the reservation cost per minute and the query duration in minutes.
This makes the estimations slightly lower than the actual cost because slots are paid also when they are idle.
However, for our purpose the difference is negligible.&lt;/p&gt;
&lt;p&gt;Firstly, to test the feasibility of this approach, I run some experiments with a smaller dataset
(just a month of data ~6.5 TB).&lt;/p&gt;
&lt;p&gt;The table below shows durations and costs for the job executed using on-demand and using different amount of slots.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Duration (min)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Cost($)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Improvement Factor&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;On-Demand&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2:26&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;32.5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;500 Slots&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5:18&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1,000 Slots&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3:05&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2,000 Slots&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1:43&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.7&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;12x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4,000 Slots&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1:06&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.9&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11x&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see from the results, using flat-rate pricing reduced the costs significantly.
In fact, the fewer slots we use, the cheaper the job becomes.
On the down side, it needs more time to complete, but this is expected.
The interesting results is that the 2,000 and 4,000 slots tests showed not only a similar decrease in cost, but also and improvement in performance.
This looks like a win-win situation!!!
We need to remember that this analysis is valid for our non complex query and it might yield very different results for more complex jobs and workloads.
I will present different scenarios and their analysis in the following blog posts.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Let&amp;rsquo;s go a bit more in the details of the experiments results.&lt;/p&gt;
&lt;p&gt;I used &lt;a href=&#34;https://bqvisualiser.appspot.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bqvisualiser&lt;/a&gt; to visualize the job progress and estimation of slots usage during execution.
The top part of the graph shows the &lt;a href=&#34;https://cloud.google.com/bigquery/docs/slots#query_execution_using_slots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;units of work&lt;/a&gt; that BigQuery needs to perform to complete the query.
The bottom part of the graph shows the &lt;strong&gt;estimated used slots&lt;/strong&gt; at a certain time during the query execution.&lt;/p&gt;


















&lt;figure id=&#34;figure-on-demand&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/small_on_demand.png&#34; data-caption=&#34;On-Demand&#34;&gt;


  &lt;img src=&#34;./images/small_on_demand.png&#34; alt=&#34;small_on_demand&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    On-Demand
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;When using on-demand pricing our slots usage profile shows that it cruises around 2,000 slots, but sometimes it bursts to 4,000 slots (however, it often uses fewer slots).
This happens because when using on-demand pricing, the available slots are (loosely) &amp;ldquo;capped&amp;rdquo; to 2,000 slots per project.
However, projects might access to more slots in burst from time to time.&lt;/p&gt;
&lt;p&gt;The slots usage profile radically change when running the query under a reservation.
It is clear that the query fully utilizes the available slots for almost 100% of the execution time.
This is particularly true for the 500 slots test:&lt;/p&gt;


















&lt;figure id=&#34;figure-500-slots&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/small_slots_500.png&#34; data-caption=&#34;500 Slots&#34;&gt;


  &lt;img src=&#34;./images/small_slots_500.png&#34; alt=&#34;small_slots_500&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    500 Slots
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;It is clear that this query uses all the available slots except for 3 parts: the first 2 dips and the tail.
The first 2 dips are linked to a repartition and a consequent re-aggregation stages.
Meanwhile, the tail is due to some workers needing more time to complete their writes.&lt;/p&gt;
&lt;p&gt;

















&lt;figure id=&#34;figure-1000-slots&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/small_slots_1000.png&#34; data-caption=&#34;1,000 Slots&#34;&gt;


  &lt;img src=&#34;./images/small_slots_1000.png&#34; alt=&#34;small_slots_1000&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    1,000 Slots
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-2000-slots&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/small_slots_2000.png&#34; data-caption=&#34;2,000 Slots&#34;&gt;


  &lt;img src=&#34;./images/small_slots_2000.png&#34; alt=&#34;small_slots_2000&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    2,000 Slots
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-4000-slots&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/small_slots_4000.png&#34; data-caption=&#34;4,000 Slots&#34;&gt;


  &lt;img src=&#34;./images/small_slots_4000.png&#34; alt=&#34;small_slots_4000&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    4,000 Slots
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In general, more slots make the job faster (as long as BigQuery can divide the job in enough work units).
However, you can see that the job parts that do not fully utilise a small slots reservation will not use more slots even when the reservation is larger.&lt;/p&gt;
&lt;p&gt;Moreover, the final cost is consistently reduced, even if we end up paying for idling slots.&lt;/p&gt;
&lt;h2 id=&#34;go-big&#34;&gt;Go Big&lt;/h2&gt;
&lt;p&gt;Now let&amp;rsquo;s see how the full job performed.
This time, I will not run the job using on-demand pricing for obvious reasons
💸💸💸💸💸.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Duration (min)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Cost($)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Improvement Factor&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;On-Demand&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;ndash;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;425&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;500 Slots&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;75&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;25&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1,000 Slots&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;40&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;25&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2,000 Slots&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;25&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;33&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4,000 Slots&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;14&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11x&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;

















&lt;figure id=&#34;figure-500-slots&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/all_time_slots_500.png&#34; data-caption=&#34;500 Slots&#34;&gt;


  &lt;img src=&#34;./images/all_time_slots_500.png&#34; alt=&#34;all_time_slots_500&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    500 Slots
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-1000-slots&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/all_time_slots_1000.png&#34; data-caption=&#34;1,000 Slots&#34;&gt;


  &lt;img src=&#34;./images/all_time_slots_1000.png&#34; alt=&#34;all_time_slots_1000&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    1,000 Slots
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-2000-slots&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/all_time_slots_2000.png&#34; data-caption=&#34;2,000 Slots&#34;&gt;


  &lt;img src=&#34;./images/all_time_slots_2000.png&#34; alt=&#34;all_time_slots_2000&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    2,000 Slots
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-4000-slots&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./images/all_time_slots_4000.png&#34; data-caption=&#34;4,000 Slots&#34;&gt;


  &lt;img src=&#34;./images/all_time_slots_4000.png&#34; alt=&#34;all_time_slots_4000&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    4,000 Slots
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can see the same pattern in the estimated slots usage graph.
In fact, you can see the same the 2 dips and the tail.&lt;/p&gt;
&lt;p&gt;In our case, the query has only a few and short dips and tail.
So, we could continue improving performance by simply buying more slots.
You can see that when we test 4,000 slots the job complete faster than with 2,000, but it costs more.
This is because the time we save adding more slots is not enough to compensate the idle slots time.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, flat-rate pricing proved to be enormously beneficial to reduce the cost of our data-intensive job.
However, beforehand it can be tricky to decide when to use flat-rate or on-demand pricing for a job.
Intuitively, we can say that a good candidate for flat-rate pricing is a job that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Will scan lots of data (it will cost lots of money using on-demand pricing).&lt;/li&gt;
&lt;li&gt;Has a very consistent usage of slots during its execution time.&lt;/li&gt;
&lt;li&gt;The job doesn&amp;rsquo;t have strict time completion requirements.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stay tuned for more in depth experiments and analysis on BigQuery slots and different workloads evaluation.&lt;/p&gt;
&lt;p&gt;Follow or contact me on Twitter for more stories about data, infrastructure, and automation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BigQuery Authorized Views with Terraform</title>
      <link>https://alepuccetti.github.io/post/bigquery-authorized-views-with-terraform/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/bigquery-authorized-views-with-terraform/</guid>
      <description>

















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./bq_tf.png&#34; &gt;


  &lt;img src=&#34;./bq_tf.png&#34; alt=&#34;BigQuery &amp;#43; Terraform&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;In one of my previous blog posts, we have seen how, at &lt;a href=&#34;https://huq.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huq Industries&lt;/a&gt;, we used authorised views to reduce costs, complexity, and delivery time.
You can read more about it &lt;a href=&#34;https://alepuccetti.github.io/post/bigquery-real-time-data-delivery-at-scale/&#34;&gt;here&lt;/a&gt;.
In this post, we will see how to implement authorised views in production and managing them as code.
This solution enables us to easily manage 100s of clients, each one with unique data requirements.
To better manage and automate our workflow, we source control and review all our Google Cloud Platform data infrastructure using GitHub and Terraform Cloud.
This enabled us to easily communicate, track, and maintain infrastructure changes.
Let’s see how to define views and authorize them in the needed datasets.
First, we would need a &lt;strong&gt;dataset&lt;/strong&gt; in the client project to contain a view, then we would need the &lt;strong&gt;view&lt;/strong&gt; itself, and finally the &lt;strong&gt;authorization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Define the &lt;strong&gt;dataset&lt;/strong&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/87a74e9469a612271d9adda8676eef50.js?file=define_user_dataset.tf&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Define the &lt;strong&gt;view&lt;/strong&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/87a74e9469a612271d9adda8676eef50.js?file=define_user_view.tf&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Lastly, we have to &lt;strong&gt;authorise&lt;/strong&gt; the view to access any needed dataset (in this example just one)
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/87a74e9469a612271d9adda8676eef50.js?file=authorize_user_view.tf&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;After having generated and checked the plan you can apply it.
The view will then be created so that each user who has access to it can run queries without having direct access to the underlying tables.&lt;/p&gt;
&lt;p&gt;To easily scale this to 100s of clients, we can write a terraform module that implements the last 2 steps and invoke it for each client we need to provision with just different variables such as: &lt;code&gt;client_project_id&lt;/code&gt;, &lt;code&gt;client_dataset_name&lt;/code&gt;, &lt;code&gt;view_name&lt;/code&gt;, &lt;code&gt;view_query&lt;/code&gt;, and the list of &lt;code&gt;project_id&lt;/code&gt;, &lt;code&gt;datase_id&lt;/code&gt;, to autorise.&lt;/p&gt;
&lt;p&gt;One of the nicest things of authorised views is that the authorisation works as a chain.
At Huq, we leverage this behaviour so that we just have to authorise the clients views to the dataset that directly accesses them.
We use authorised views almost everywhere so that if one of these views are used by some other views, the end user does not need extra authorisation.&lt;/p&gt;
&lt;h3 id=&#34;warning&#34;&gt;WARNING&lt;/h3&gt;
&lt;p&gt;If you also manage datasets access via other terraform resources such as: &lt;code&gt;bigquery_dataset_iam_policy&lt;/code&gt;, &lt;code&gt;bigquery_dataset_iam_binding&lt;/code&gt;, or &lt;code&gt;bigquery_dataset_iam_member&lt;/code&gt;.
They will fight over what the policy should be with &lt;code&gt;google_bigquery_dataset_access&lt;/code&gt;.
In fact, using any of the &lt;code&gt;google_bigquery_dataset_*&lt;/code&gt; resources will result in removing any authorised views from that dataset previously configured via &lt;code&gt;google_bigquery_dataset_access&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can read more about this in the terraform &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Follow or contact me on Twitter for more stories about data, infrastructure, and automation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-Time data delivery at scale with BigQuery</title>
      <link>https://alepuccetti.github.io/post/bigquery-real-time-data-delivery-at-scale/</link>
      <pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/bigquery-real-time-data-delivery-at-scale/</guid>
      <description>

















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./data_delivery.jpeg&#34; &gt;


  &lt;img src=&#34;./data_delivery.jpeg&#34; alt=&#34;Data Delivery&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;At &lt;a href=&#34;https://huq.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huq Industries&lt;/a&gt;, we collect and process 30 billions events a month from more than 8 million devices.
Every day, we enrich, slice, and then deliver data feeds in various forms to our clients mainly via BigQuery, GCS, or S3.
&lt;br&gt;In the past 2 years, we have seen our data ingestion growing 4x each year and we forecast the same for the next years to come.
Our already large data history and its steady and high grow rate pose several challenges from ingestion to storage, from processing to delivery and others.
&lt;br&gt;All these parts can be very critical.
However, in this specific post, we will focus on the processing and delivery.
We will see how we leveraged &lt;a href=&#34;https://cloud.google.com/bigquery/docs/authorized-views&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BigQuery Authorized Views&lt;/a&gt; to cut our storage and processing costs, to reduce our delivery time to zero and to streamline the whole system by removing many processes and dependencies.&lt;/p&gt;
&lt;h2 id=&#34;the-issue&#34;&gt;The issue&lt;/h2&gt;
&lt;p&gt;Many of our clients access our data directly using our self-served analytics offering via BigQuery sandboxed instances.
&lt;br&gt;At the very beginning, our system was creating and updating tables for each individual client in its own instances.
This choice was based on how the core processing system created and updated the master tables (one table per country updated daily).
This soon resulted  in a lot of duplicated data which, in turn, was increasing our costs with the data growth and the number of clients.
&lt;br&gt;Hence, we &lt;strong&gt;needed&lt;/strong&gt; to find a different design before it would have become unsustainable.&lt;/p&gt;
&lt;h2 id=&#34;the-solution-authorized-views-everywhere&#34;&gt;The solution: Authorized Views Everywhere&lt;/h2&gt;
&lt;p&gt;Now that we leveraged BigQuery clustering (see previous &lt;a href=&#34;https://alepuccetti.github.io/post/bigquery-partitioning-clustering/&#34;&gt;blogpost&lt;/a&gt;), instead of sharding data into different tables for each country, we were able to simplify part of the data processing and delivery system removing ETL jobs and dependencies.
This resulted in &lt;strong&gt;cost reduction&lt;/strong&gt; for both processing and storage.
Last, but definitely not least, it removed the delay in delivery time from when the data is ready to when the data is available to the clients.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is an authorised view?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An authorised view is a SQL view that was &lt;strong&gt;authorised&lt;/strong&gt; to read tables in a specific dataset.
This means that users, who are allowed to access the view, do not need to get permission  to access the underlined tables.
It is the view itself that holds that permission and acts as a sort of middle-man.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How is this useful for us?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In our case, we were able to provide all our clients with a view that grants them access only to the data they are subscribing to.
Each view can be different and customized according to each client&amp;rsquo;s needs but all of them use the same source tables.
Thus, removing a lot of duplicated data and updating processes.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Now, let’s assume that we have 10s of clients who subscribed to different timeframes and/or countries to our data.
Additionally, some of them needed customization of the table schema.
As you can imagine, the ETL jobs and their scheduling become quite complicated and pricey. Clearly, this won’t be sustainable when scaling up to &lt;strong&gt;100s&lt;/strong&gt; or &lt;strong&gt;1000s&lt;/strong&gt; of clients.
&lt;br&gt;&lt;strong&gt;Authorised views&lt;/strong&gt; enabled us to optimize this issue and we can now easily serve any number of clients with custom data requirements with very little overhead.
This can be done by simply creating a view and authorising it to the master dataset:&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/d608ebaac5995eaf382074911012cacf.js?file=create_view.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;After we created this view and we granted users access to it, they still won’t be able to successfully run their queries.
In fact, if a user does not have access to the underlying tables (i.e. &lt;code&gt;table_a&lt;/code&gt; and &lt;code&gt;table_b&lt;/code&gt;) and he/she runs a query using this view, that will result in a permission error.
&lt;br&gt;However, by authorizing the view to access &lt;code&gt;dataset_a&lt;/code&gt; and &lt;code&gt;dataset_b&lt;/code&gt; that will enable users to successfully run their queries.
&lt;br&gt;This can easily be done in the BigQuery explorer. To get to the “authorised views” menu go to &lt;code&gt;source_project_id&lt;/code&gt; ➝ &lt;code&gt;dataset_a&lt;/code&gt; ➝ &amp;ldquo;Share Dataset&amp;rdquo; ➝ &amp;ldquo;Authorised Views&amp;rdquo;. Now, you can configure which views have access to this dataset using their &lt;code&gt;project_id&lt;/code&gt;, &lt;code&gt;dataset_id&lt;/code&gt;, and &lt;code&gt;view_id&lt;/code&gt;. In our example: &lt;code&gt;client_project&lt;/code&gt;, &lt;code&gt;dataset&lt;/code&gt;, and  &lt;code&gt;view_name&lt;/code&gt;. Repeat this for all the dataset that the view needs to access, in our example we have 2 datasets in 2 different projects.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;bonus-views-using-views&#34;&gt;&lt;strong&gt;Bonus&lt;/strong&gt;: views using views&lt;/h2&gt;
&lt;p&gt;BigQuery, before running a query, resolves all the views and finds all the source tables and verifies that the user or, in our case, the view can read from all them.
Otherwise will return a permission error.
&lt;br&gt;What we saw so far works well if our views use tables. &lt;strong&gt;What if our authorised view uses another view?&lt;/strong&gt; Would this solution still work as is? Does our view need to be authorised on all the dataset used by all the used views and their views etc.?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The answer is no&lt;/strong&gt;, at least not directly.&lt;/p&gt;
&lt;p&gt;Of course you could authorise the final view to access all of these tables but, as you can imagine, this will result in a lot of dependencies, repetitions, and complex definitions that will be a nightmare to maintain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can we successfully use views in an authorised view?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;authorised views&lt;/strong&gt;. More precisely, we can use authorised views in another authorised view because the “inner” view will be authorised to access the resources creating a chain of authorisation. In this way, we can just simply manage the authorisation of each single view.&lt;/p&gt;
&lt;p&gt;Stay tuned for an implementation of this using Terraform in the coming blogposts.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Follow or contact me on Twitter for more stories about data, infrastructure, and automation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Time Travel with BigQuery</title>
      <link>https://alepuccetti.github.io/post/bigquery-time-travel/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/bigquery-time-travel/</guid>
      <description>

















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./time_bending.jpeg&#34; &gt;


  &lt;img src=&#34;./time_bending.jpeg&#34; alt=&#34;Time bending&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Who does not like time travel? We all saw it and were fascinated by it in many sci-fi movies, unfortunately science did not crack real-life time travel yet.&lt;/p&gt;
&lt;p&gt;However, we can “data time-travel”. Thanks to the amazing BigQuery SQL feature “FOR SYSTEM_TIME AS OF”, we can time travel (up to 7 days in the past) to a specific timestamp (for detailed information about the syntax refer to the &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#for_system_time_as_of&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official documentation&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This feature is extremely easy to use. In fact, we can simply modify any query by adding the  clause above mentioned to the FROM clause. The following example returns all rows in  &lt;code&gt;table_a&lt;/code&gt; from exactly 3 hours ago.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/277fe07f200ccac33f98ae4217740fa5.js?file=time_travel_query_interval.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;Or we can specify a precise point-in-time.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/277fe07f200ccac33f98ae4217740fa5.js?file=time_travel_query_point_in_time.sql&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;how-is-this-useful&#34;&gt;How is this useful?&lt;/h2&gt;
&lt;p&gt;Ok, it is nice to talk about time travel, but how is this useful in real-life?
In the current age of high data integration, data is inserted, deleted, and modified at a high rate. In highly integrated and streaming systems, it is often challenging to plan and perform backups that can be used to recover from processing errors. In BigQuery this can be done by just selecting rows from a specific point-in-time. This removes the need to materialize snapshots (for the past 7 days) and give us incredible granularity (timestamp precision).
The most obvious use-case is &lt;strong&gt;data recovery&lt;/strong&gt;. We can easily recover a table state by creating a new one from a specific point-in-time.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/277fe07f200ccac33f98ae4217740fa5.js?file=recover_table_point_in_time.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;A more interesting and less obvious use-case could be to ensure &lt;strong&gt;consistent analytics&lt;/strong&gt; results over time from a table that is modified with a high frequency or that  receives streaming inserts.
In this case, we could configure the SQL query to always return results from a specific timestamp during a specific time frame. We could build a caching system to do that or we could use “FOR SYSTEM_TIME AS OF” with a specific point-in-time to “force” queries to always return results from a past table state and just change the value when we want the results from a different time.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/277fe07f200ccac33f98ae4217740fa5.js?file=point_in_time_analytics.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;Or for a more dynamic solution&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/277fe07f200ccac33f98ae4217740fa5.js?file=delayed_analytics.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;Follow or contact me on Twitter for more stories about data, infrastructure, and automation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Serverless S3 Access Logs Analytics using BigQuery</title>
      <link>https://alepuccetti.github.io/post/serverless-s3-access-logs-analytics-with-bigquery/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/serverless-s3-access-logs-analytics-with-bigquery/</guid>
      <description>

















&lt;figure id=&#34;figure-multi-cloud-data-analytics&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./man_binoculars.jpg&#34; data-caption=&#34;Multi-cloud Data Analytics&#34;&gt;


  &lt;img src=&#34;./man_binoculars.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Multi-cloud Data Analytics
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Do you use BigQuery? Are you interested in knowing how to integrate data from different cloud providers into BigQuery? In this blogpost, we will implement a &lt;strong&gt;serverless&lt;/strong&gt; and &lt;strong&gt;fully managed&lt;/strong&gt; system to make available S3 access logs into BigQuery to easily integrate them with other data sources and reporting systems. To achieve this we will see how to set up AWS S3 access logs delivery and configure Google &lt;a href=&#34;https://cloud.google.com/storage-transfer/docs/create-manage-transfer-console#amazon-s3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Transfer Service&lt;/a&gt; in order to schedule fully managed S3 to Cloud Storage transfers.We will also use &lt;a href=&#34;https://cloud.google.com/bigquery/external-table-definition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BigQuery external table&lt;/a&gt; to read data directly from Google Cloud Storage to access always to the most recent data without keeping reloading data into BigQuery. Finally, we will leverage &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions#temporary-udf-syntax&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;persistent UDFs&lt;/a&gt; to parse the logs into tabular format on the fly.&lt;/p&gt;
&lt;h2 id=&#34;set-up-s3-access-logging&#34;&gt;Set up S3 access logging&lt;/h2&gt;
&lt;p&gt;AWS allows you to export &lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S3 access logs&lt;/a&gt;, which can be used to monitor user access and build business metrics. Although, you can export logs for a specific bucket into itself, this might cause extra logs to get generated. In our scenario we use a different bucket to collect and store the access logs of many other buckets. Also, S3 access logs can be exported only to a bucket in the same region for the monitored one. So we will need to create a different bucket for each region where we have buckets that we want to monitor. In this scenario, we will work in the &amp;ldquo;eu-west-1&amp;rdquo; region. Let’s create a bucket to collect and store the access logs.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;aws s3api create-bucket --bucket MY_BUCKET_EU-WEST-1 --region eu-west-1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to set up S3 access logs delivery. First we grant the AWS LogDelivery group the relevant permissions to our destination bucket:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;aws s3api put-bucket-acl --bucket MY_BUCKET_EU-WEST-1 --grand-write URI=http://acs.amazonaws.com/groups/s3/LogDelivery --grant-read-acp URI=http://acs.amazonaws.com/groups/s3/LogDelivery
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we define the logging configuration locally in a JSON file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;LoggingEnabled&amp;quot;: {
    &amp;quot;TargetBucket&amp;quot;: &amp;quot;MY_BUCKET_EU-WEST-1&amp;quot;,
    &amp;quot;TargetPrefix&amp;quot;: &amp;quot;BUCKET_TO_LOG/&amp;quot;,
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;TargetBucket&lt;/code&gt; is the bucket where the logs will be delivered. You can provide a prefix for the logs using &lt;code&gt;TargetPrefix&lt;/code&gt; (optional), we use it to separate the logs of different buckets but you could use a different logic. Now we need to enable logging:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;aws s3api put-bucket-logging --bucket BUCKET_TO_LOG --bucket-logging-status file://logging.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s worthy to remember that logs are delivered on a &lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html#LogDeliveryBestEffort&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;best effort basis&lt;/a&gt;, usually most log records are delivered within a few hours of the time they are recorded, but can be more frequently.&lt;/p&gt;
&lt;h2 id=&#34;transfer-logs-to-google-cloud-platform&#34;&gt;Transfer logs to Google cloud Platform&lt;/h2&gt;
&lt;p&gt;In our scenario, we will transfer the data first into Cloud Storage using the &lt;a href=&#34;https://cloud.google.com/storage-transfer/docs/create-manage-transfer-console#amazon-s3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Transfer&lt;/a&gt; service and then using &lt;a href=&#34;https://cloud.google.com/bigquery/external-table-definition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BigQuery external table&lt;/a&gt; to run queries on them. Another possible solution would be to use &lt;a href=&#34;https://cloud.google.com/bigquery-transfer/docs/s3-transfer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S3 to BigQuery&lt;/a&gt; data transfer service to directly transfer the data into BigQuery.
The Data Transfer service is fully managed, so you just need to configure it and you won’t need to provision and monitor any infrastructure. Transfers are scheduled to run every 24 hours, if you need a different interval you will have to implement the data transfer yourself instead of using this service. Before setting up the transfer we need a destination bucket on Cloud Storage. Let’s create a multi-regional bucket in Europe.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gsutil mb -l EU gs://MY_AWS_S3_ACCESS_LOGS_MIRROR/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To set up the transfer service, log into your cloud console to find &lt;em&gt;Data Transfer&lt;/em&gt; under the &lt;em&gt;Storage&lt;/em&gt; section. Now click on &amp;ldquo;Create a transfer job&amp;rdquo; and follow the steps below:
As a source we select &amp;ldquo;Amazon S3 bucket and include the bucket name we want to read from, in our example &amp;ldquo;MY-S3-ACCESS-LOGS-EU-WEST-1&amp;rdquo;. We need to provide AWS authentication Access key ID and Secret access key that have the permission to read the bucket.
As the destination, we put the bucket we just created MY_AWS_S3_ACCESS_LOGS_MIRROR and leave all the other options un-ticked.
We select to run daily at a specific time.
Click on Create.&lt;/p&gt;
&lt;p&gt;For more detailed information refer to the &lt;a href=&#34;https://cloud.google.com/storage-transfer/docs/create-manage-transfer-console#amazon-s3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We still need to be able to access the data in BigQuery to do our analytics and build reporting dashboards with Data Studio. Unfortunately, AWS S3 log entries are not in JSON or CSV-like format but they use their &lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;own&lt;/a&gt;. For this reason, BigQuery cannot automatically load it into a table with the desired schema. To have the logs parsed as we desire we will use a persistent UDF but first we need to make the log entries &amp;ldquo;available&amp;rdquo; to BigQuery. To do so, we define an external table that reads data from our mirror bucket in Cloud Storage and we have to configure it as the files use newline as field separator. As a result, we want a table where each row contains an entire log entry, we choose the newline character as field separator because we do not expect to have any of them in our log entries. This will result in a table with a single column.
To create our source table we can use the cloud console:
Select the dataset you want to create your table in and click create table.
In the table creation menu, select as source &amp;ldquo;Google Cloud Storage&amp;rdquo;, CSV as format, and add the prefix of the data that you want to be the sources of the table (remember that you can use wildcard to say to BigQuery to select all the files within the prefix).
Choose your table name (in this example we will use &amp;ldquo;s3_raw_logs_external&amp;rdquo;) and because we do not want to actually load the data, we need to select &amp;ldquo;External table&amp;rdquo; in &amp;ldquo;Table type&amp;rdquo;.
In the schema section add a single column called &amp;ldquo;text&amp;rdquo; of type STRING.
Click on &amp;ldquo;Advanced options&amp;rdquo; and set the field delimiter to be &amp;ldquo;Custom&amp;rdquo; and insert s character that should never be present (e.g. &amp;ldquo;§&amp;rdquo;). You can leave the other options with their default values and click &amp;ldquo;create&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Now we have a table into BigQuery where the source data is in Cloud Storage, this means that every query reading from this table will always acces all the data present in the bucket at that time. Note that this setup might start having degraded performances for very large logs datasets.&lt;/p&gt;
&lt;h2 id=&#34;transform-raw-logs-into-tabular-format&#34;&gt;Transform raw logs into tabular format&lt;/h2&gt;
&lt;p&gt;As we said above , AWS S3 log entries are not in JSON or CSV-like format but they use their own format, so we need to parse them ourselves . We can easily do this directly in BigQuery using a persistent UDF. I wrote a Javascript UDF to parse most of the log entry parts into an object. Note that we use &amp;ldquo;\&amp;rdquo; instead of &amp;ldquo;&amp;quot; because we need to escape when defining the function.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/9560b606419a1ba1ac7235f079b4d802.js?file=s3_log_parser.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;You can copy and paste the code above into the BigQuery UI to create the &lt;strong&gt;s3_log_parser&lt;/strong&gt; as a persistent UDF, just replace &lt;code&gt;PROJECT_ID&lt;/code&gt; and &lt;code&gt;DATASET_ID&lt;/code&gt; with valid values for your environment.&lt;/p&gt;
&lt;p&gt;Now we have easy access to the data and the logic to transform it into tabular format, the following query will parse all the logs entry into the column defined in the UDF.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/9560b606419a1ba1ac7235f079b4d802.js?file=parsed_s3_logs.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;Let’s copy and paste this query into the BigQuery UI and run it. If you have already data into the Cloud Storage bucket you will see the logs parsed out. Now click on &amp;ldquo;Save view&amp;rdquo;, choose a destination dataset and a name for the view (e.g. s3_logs_parsed). In this way, we can use the view as a source for our analytics reporting dashboards instead of always writing the query itself. Now try the following query, it should show the same results of the previous one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;Select
  *
from `PROJECT_ID.DATASET_ID.s3_logs_parsed`
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you have a serverless, fully-managed system to analyse your AWS S3 access logs into BigQuery using SQL and play with Google &lt;a href=&#34;https://datastudio.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Datastudio&lt;/a&gt; to build and share reporting dashboards.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BigQuery Geography Clustering</title>
      <link>https://alepuccetti.github.io/post/bigquery-geography-clustering/</link>
      <pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/bigquery-geography-clustering/</guid>
      <description>

















&lt;figure id=&#34;figure-world-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./world-map-colored-countries.png&#34; data-caption=&#34;World Map&#34;&gt;


  &lt;img src=&#34;./world-map-colored-countries.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    World Map
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The BigQuery team rolled out support for &lt;a href=&#34;https://cloud.google.com/bigquery/docs/gis-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;geography type&lt;/a&gt; a while ago and
they have never stopped improving performances and &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/standard-sql/geography_functions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Geographic Information System functions&lt;/a&gt; (GIS).
This allows users to run &lt;em&gt;complex geo-spatial analytics&lt;/em&gt; directly in BigQuery harnessing all its power, simplicity, and reliability.&lt;/p&gt;
&lt;p&gt;Hold on your keyboard (or your screen if you are reading this on a mobile device).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now you can cluster tables using a geography column&lt;/strong&gt;. Say what!!!!&lt;/p&gt;
&lt;p&gt;This is game changing for users working heavily with geodata.
By clustering your table on a geography column, BigQuery can reduce the amount of data that needs to read to serve the query.
This makes queries cheaper and run faster when filtering on clustering column.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see the benefits of clustering table using geography column with an example.
We will use one of the great public datasets curated by &lt;a href=&#34;https://twitter.com/felipehoffa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Felipe Hoffa&lt;/a&gt;.
We will use the &lt;em&gt;weather_gsod&lt;/em&gt; specifically the two tables &lt;em&gt;all&lt;/em&gt; and &lt;em&gt;all_geoclustered&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say that we want the all-time minimum and maximum temperature within the Greater London area for each station.
Our query will look like this:&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/24eff1b26520bd842889d44042d24460.js?file=query.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;Results:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-csv&#34;&gt;name                min_temp  max_temp
ST JAMES PARK       26.3      83.2
KENLEY AIRFIELD     18.0      82.8
HEATHROW            18.6      83.4
CITY                22.3      92.2
BLACKWALL           37.0      62.2
NORTHOLT            18.4      84.1
BIGGIN HILL         15.5      90.3
PURLEY OAKS         23.0      81.7
LEAVESDEN           22.3      89.1
LONDON WEA CENTER   20.0      85.2
KEW-IN-LONDON       23.3      82.4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This query reads &lt;strong&gt;9.02GB&lt;/strong&gt; of data.
Now let&amp;rsquo;s see how the same query perform on the clustered table:&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/24eff1b26520bd842889d44042d24460.js?file=query_geo_filtered.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;The result is obviously the same but, this time, BigQuery reads just &lt;strong&gt;98.97MB&lt;/strong&gt; of the data!!!
So switching to the geo clustered table made this query almost &lt;strong&gt;100 times&lt;/strong&gt; cheaper.
Do you want to try other locations and test the differences yourself? You can use Huq Industries &lt;a href=&#34;https://gismap.huq.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GisMap tool&lt;/a&gt; to quickly draw and export polygons in various formats.&lt;/p&gt;


















&lt;figure id=&#34;figure-huq-gismaphttpgismaphuqio&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./huq_gismap_london_m25.jpeg&#34; data-caption=&#34;&amp;lt;a href=&amp;#34;http://gismap.huq.io/&amp;#34; target=&amp;#34;_blank&amp;#34; rel=&amp;#34;noopener&amp;#34;&amp;gt;Huq GisMap&amp;lt;/a&amp;gt;&#34;&gt;


  &lt;img src=&#34;./huq_gismap_london_m25.jpeg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;a href=&#34;http://gismap.huq.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Huq GisMap&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>BigQuery Partitioning &amp; Clustering</title>
      <link>https://alepuccetti.github.io/post/bigquery-partitioning-clustering/</link>
      <pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/bigquery-partitioning-clustering/</guid>
      <description>

















&lt;figure id=&#34;figure-shipping-containers&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./shipping_containers.jpg&#34; data-caption=&#34;Shipping Containers&#34;&gt;


  &lt;img src=&#34;./shipping_containers.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Shipping Containers
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In this blogpost, I will explain what partitioning and clustering features in BigQuery are and
how to supercharge your query performance and reduce query costs.&lt;/p&gt;
&lt;h1 id=&#34;partitioning&#34;&gt;Partitioning&lt;/h1&gt;
&lt;p&gt;Partitioning a table can make your queries run faster while spending less.
Until December 2019, BigQuery supported table partitioning only using &lt;strong&gt;date data type&lt;/strong&gt;.
Now, you can do it on integer ranges too.
If you want to know more about partitioning your tables this way,
check out this &lt;a href=&#34;https://medium.com/google-cloud/partition-on-any-field-with-bigquery-840f8aa1aaab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great blogpost&lt;/a&gt; by Guillaume Blaquiere.&lt;/p&gt;
&lt;p&gt;Here, I will focus on date type partitioning.
You can partition your data using 2 main strategies:
on the one hand you can use a table column, and on the other, you can use the data time of ingestion.&lt;/p&gt;
&lt;p&gt;This approach is particularly useful when you have very large datasets that go back in time for many years.
In fact, if you want to run analytics only for specific time periods, partitioning your table by time allows BigQuery
to read and process only the rows of that particular time span.
Thus, your queries will run faster and, because they are reading less data, they will also cost less.&lt;/p&gt;
&lt;p&gt;Creating a partitioned table is an easy task.
At the time of table creation, you can specify which column is going to be used for partitioning,
otherwise, you can set up the partitioning on &lt;a href=&#34;https://cloud.google.com/bigquery/docs/creating-partitioned-tables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ingestion time&lt;/a&gt;.
Since you can query this table in the same exact way of those that are not partitioned, you won&amp;rsquo;t have to change a line of your existing queries.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/859d196d00588efcfb08724218cfabaf.js?file=query_partition.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;Assuming that &amp;ldquo;&lt;strong&gt;sampling_date&lt;/strong&gt;&amp;rdquo; is the partitioning column,
now BigQuery can use the specified values in the &amp;ldquo;where clause&amp;rdquo; to read only data that belong to the right partitions.&lt;/p&gt;
&lt;h4 id=&#34;bonus-nugget&#34;&gt;Bonus nugget&lt;/h4&gt;
&lt;p&gt;You can use &lt;strong&gt;partition decorators&lt;/strong&gt; to update, delete, and overwrite entire single partitions as in:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# overwrite single partition loading from file
bq load —-replace \
  project_id:dataset_name.table_name$20190805 \
  gs://my_input_bucket/data/from/20190805/* ./schema.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# overwrite single partition from query results
bq query —- replace --use_legacy_sql=false \
  —-destination_table project_id:dataset.table$20190805 \
  &#39;select * from project_id:dataset.another_table&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the cases above, both the loaded data and the query results have to belong to the referenced partition, otherwise the job will fail.&lt;/p&gt;
&lt;h1 id=&#34;clustering&#34;&gt;Clustering&lt;/h1&gt;
&lt;p&gt;Clustering is another way of organizing data which stores one next to the other all those rows that share similar values in the chosen clustering columns.
This process increases the query efficiency and performances.
Note that BigQuery supports this feature only on partitioned tables.&lt;/p&gt;
&lt;p&gt;BigQuery can leverage clustered tables to read only data relevant to the query, so it becomes faster and cheaper.&lt;/p&gt;
&lt;p&gt;At the table creation time, you can provide up to 4 clustering columns in a comma-separated list e.g. &amp;ldquo;&lt;strong&gt;wiki&lt;/strong&gt;&amp;rdquo;, &amp;ldquo;&lt;strong&gt;title&lt;/strong&gt;&amp;rdquo;.
You should also keep in mind that their order is of paramount importance but we will see this in a moment.&lt;/p&gt;
&lt;p&gt;In this section we will use &amp;ldquo;&lt;em&gt;wikipedia_v3&lt;/em&gt;&amp;rdquo; form &lt;a href=&#34;https://twitter.com/felipehoffa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Felipe Hoffa&lt;/a&gt;&amp;rsquo;s public dataset,
which contains yearly tables of Wikipedia page views.
These are partitioned by the &amp;ldquo;&lt;strong&gt;datehour&lt;/strong&gt;&amp;rdquo; column and clustered on &amp;ldquo;&lt;strong&gt;wiki&lt;/strong&gt;&amp;rdquo; and &amp;ldquo;&lt;strong&gt;title&lt;/strong&gt;&amp;rdquo; columns.
A single row may look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-csv&#34;&gt;datehour,                    language,     title,   views
2019–08–10 03:00:00 UTC,     en,           Pizza,   106
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following query counts, broken-down per year, all the page views for the Italian wiki from 2015–01–01.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/859d196d00588efcfb08724218cfabaf.js?file=IT_query_cluster.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;If you write this query in BigQuery UI, it will estimate a data scanning of 4.5 TB.
However, if you actually run it, the final scanned data will be of just 160 GB.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How is this possible?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When BigQuery reads only read rows belonging to the cluster that contains the data for the Italian wiki while discarding everything else.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Why is the columns order so important in clustering?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is important because BigQuery will organize the data hierarchically according to the column order that is specified when the table is created.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use the following example:&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/859d196d00588efcfb08724218cfabaf.js?file=pizza_query_cluster.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;This query needs to access all the &amp;ldquo;wiki&amp;rdquo; clusters and then it can use the &amp;ldquo;title&amp;rdquo; value to skip the not matching clusters.
This results in scanning a lot more data than if the clustering columns were in the opposite order &amp;ldquo;&lt;strong&gt;title&lt;/strong&gt;&amp;rdquo;, &amp;ldquo;&lt;strong&gt;wiki&lt;/strong&gt;&amp;rdquo;.
At the time of writing, the query above estimated a scanning cost of 1.4 TB but it actually scanned only 875.6 GB of data.
Let&amp;rsquo;s now invert the clustering columns order putting first &amp;ldquo;&lt;strong&gt;title&lt;/strong&gt;&amp;rdquo; and second &amp;ldquo;&lt;strong&gt;wiki&lt;/strong&gt;&amp;rdquo;, you can do so using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bq query --allow_large_results --nouse_legacy_sql \
  --destination_table my_project_id:dataset_us.wikipedia_2019 \
  --time_partitioning_field datehour \
  --clustering_fields=title,wiki \
&#39;select * from `fh-bigquery.wikipedia_v3.pageviews_2019`&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running the &amp;ldquo;Pizza&amp;rdquo; query on our new table &amp;ldquo;&lt;strong&gt;my_project_id:dataset_us.wikipedia_2019&lt;/strong&gt;&amp;rdquo; should be much cheaper.
In fact, while the estimation was still of 1.4 TB, the actual data read was just of 26.3 GB, that is 33 times less.&lt;/p&gt;
&lt;p&gt;As final test let&amp;rsquo;s try filtering on the &amp;ldquo;&lt;strong&gt;wiki&lt;/strong&gt;&amp;rdquo; column:&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/859d196d00588efcfb08724218cfabaf.js?file=IT_query_cluster_after_re_cluster.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;The data read estimation is always the same but now the actually data read jumped to 1.4 TB (the entire table)
whereas, in the first example, the actually data read was just of 160 GB.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Since BigQuery uses a columnar store, &amp;ldquo;&lt;strong&gt;title is not null&lt;/strong&gt;&amp;rdquo; ensures that we refer always to the same number of columns in every query.
Otherwise, the data read from the last query is lower because we refer to fewer columns.&lt;/p&gt;
&lt;p&gt;It is evident that choosing the right clustering columns and their order makes a great difference.
You should plan it accordingly to your workloads.&lt;/p&gt;
&lt;p&gt;Remember, always &lt;strong&gt;partition&lt;/strong&gt; and &lt;strong&gt;cluster&lt;/strong&gt; your tables!
It is free, it does not need to change any of your queries and it will make them cheaper and faster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BigQuery Wildcards</title>
      <link>https://alepuccetti.github.io/post/bigquery-wildcards/</link>
      <pubDate>Mon, 06 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/bigquery-wildcards/</guid>
      <description>&lt;p&gt;BigQuery supports the &amp;ldquo;*&lt;em&gt;&amp;rdquo;&lt;/em&gt; wildcard to reference multiple tables or files.
You can leverage this feature to load, extract, and query data across multiple sources, destinations, and tables.
Let&amp;rsquo;s see what you can do with wildcards with some examples.&lt;/p&gt;
&lt;p&gt;The first thing is definitely loading the data into BigQuery.
If you deal with a very large amount of data you will have, most likely,
tens of thousands of files coming from a data pipelines that you want to load into BigQuery.
Using wildcards, you can easily load data from different files into a single table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bq load project_id:dataset_name.table_name gs://my_data/input/prefix/* ./schema.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, this is not limited to only one prefix but you can specify multiple ones for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bq load project_id:dataset_name.table_name gs://my_data/input/prefix_1/* gs://my_data/input/prefix_5/* gs://my_data/input/prefix_25/* ./schema.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command above will load all the files matching all the prefixes into the specified table.&lt;/p&gt;
&lt;p&gt;Wildcards can be used in the other direction too.
Namely, they can be used to export data from BigQuery to GCS.
This is very useful especially because BigQuery limits exports to a single file only to tables smaller than 1GB.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bq extract project_id:dataset_name.table_name gs://my_data/extract/prefix/file_prefix_*.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous command will result in multiple files exported into the &lt;em&gt;&amp;ldquo;my_data&amp;rdquo;&lt;/em&gt; bucket within the prefix &lt;em&gt;&amp;ldquo;extract/prefix/&amp;rdquo;&lt;/em&gt; and all file names will be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;file_prefix_000000000000.json
file_prefix_000000000001.json
file_prefix_000000000002.json
…
file_prefix_000000003792.json
file_prefix_000000003793.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other very useful use of wildcards is evident in queries.
In fact, you can reference multiple tables in a single query by using &lt;em&gt;&amp;quot;&lt;/em&gt;&amp;quot;* to match all the table into the dataset with the same prefix.
For example, consider you have a collection of tables like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;my_dataset
├── events_GB
...
├── events_IT
...
├── events_US
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following query will return the count per day per country of events of type &lt;em&gt;&amp;ldquo;submit&amp;rdquo;&lt;/em&gt; in our dataset.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/127ea15abe46a2ffbb7e727714c6b5da.js?file=table_wildcards.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;You can also filter out matched tables using &lt;em&gt;&amp;quot;_table_suffix&amp;quot;&lt;/em&gt; in the where clause.
For example, if you are only interested in Germany, France, and Japan just run the following:&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/alepuccetti/127ea15abe46a2ffbb7e727714c6b5da.js?file=table_wildcards_filtered.sql&#34;&gt;&lt;/script&gt;

&lt;p&gt;What I personally like the most of using wildcards is that it enables me to design better, simpler,
and more generic analytics queries as well as ETL jobs.
The aim of this post was to help you improve your code quality and your productivity.&lt;/p&gt;
&lt;p&gt;If you believe you learnt something new or if you liked the post please Clap.
Happy query with BigQuery!!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Journey Through Big Data Using Google BigQuery</title>
      <link>https://alepuccetti.github.io/post/bigquery-jounrey/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/bigquery-jounrey/</guid>
      <description>

















&lt;figure id=&#34;figure-bigquery&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./bigquery_logo.svg&#34; data-caption=&#34;BigQuery&#34;&gt;


  &lt;img src=&#34;./bigquery_logo.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    BigQuery
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Back in the early days of Huq we were ingesting a just few millions records per day into our geo-behavioural insights platform.
Today that figure is in the hundreds of millions.
During the period where our traffic was ramping intensively,
we quickly realised that our single high-spec bare metal server setup was not going to be enough for our analytics needs.&lt;/p&gt;
&lt;p&gt;After all, what good is building a valuable data asset if you can&amp;rsquo;t get answers out?
We wanted to find a way to retrieve answers in seconds, not days, and so we set ourselves a mission:
find a solution that allows us to query and obtain answers to sometimes complex, often spatial,
queries across billions of records in minutes at most.
We wanted &lt;strong&gt;great performance&lt;/strong&gt;, high &lt;strong&gt;reliability&lt;/strong&gt;, to make use of our &lt;strong&gt;SQL&lt;/strong&gt; skills.
And all that with minimum set-up, and &lt;strong&gt;minimal DevOps&lt;/strong&gt; management overhead. &lt;em&gt;Moon on a stick? Definitely.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We tested different solutions but it quickly became clear that &lt;a href=&#34;https://cloud.google.com/bigquery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BigQuery&lt;/a&gt; was a step
(often several steps) ahead of the myriad other solutions that we evaluated.
We found BigQuery to be: &lt;strong&gt;effortless&lt;/strong&gt; to set up and use with virtually zero management and DevOps overhead.
It has with blazingly fast performance, flexibility and portability.
We can&amp;rsquo;t remember the last time we had to carve out a subset of our data, build indices and otherwise optimise our queries. Load your data, buckle up and go.&lt;/p&gt;
&lt;p&gt;Because it is &lt;strong&gt;fully managed&lt;/strong&gt; you get a lot of great features out-of-the-box:
high availability, &lt;strong&gt;serverless&lt;/strong&gt; computing, automatic backups, logging and auditing.&lt;/p&gt;
&lt;p&gt;Moreover, BigQuery UI is a great tool that allowed the team to get up and running fast.
BigQuery&amp;rsquo;s Standard SQL removes the need to learn a new language, or set up special DB client
(although you can use many different SQL clients to connect to BigQuery).
It also offers a rapidly expanding menu of GIS functions that in some ways are easier to work with than those found in PostGIS.
Last but not least, BigQuery offers a simple pricing model that makes it very easy to estimate cost for internal and external projects,
and to keep a firm handle on spend.&lt;/p&gt;
&lt;p&gt;Google BigQuery has dramatically improved our access to analytics and the insights that arise from our dataset.
Even relatively simple queries that in our heavily-optimised bare-metal environment still ran for several hours, on BigQuery returns seconds.
BigQuery has helped us to improve the breadth and quality of our analytical offerings through speed, complexity and the ability to iterate.
We also now serve many of our clients using BigQuery — either alone or as a backend.
BigQuery makes it possible offer direct access to our datasets to in-house data science teams, or to power specialised dashboards according to customer needs.&lt;/p&gt;
&lt;p&gt;For us, Google BigQuery has helped our business make a huge leap forward — and there&amp;rsquo;s no going back from here.
Stay tuned for more technical follow-ups on our use of Google BigQuery.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Google Cloud Composer: Overcoming The Short-living Tasks Problem</title>
      <link>https://alepuccetti.github.io/post/composer-short-living-tasks/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://alepuccetti.github.io/post/composer-short-living-tasks/</guid>
      <description>

















&lt;figure id=&#34;figure-google-cloud-composer&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./gcp_airflow_composer.png&#34; data-caption=&#34;Google Cloud Composer&#34;&gt;


  &lt;img src=&#34;./gcp_airflow_composer.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Google Cloud Composer
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/composer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cloud Composer&lt;/a&gt; is Google Cloud Platform product that helps you manage complex workflows with ease.&lt;/p&gt;
&lt;p&gt;It is built on top of &lt;a href=&#34;https://airflow.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Airflow&lt;/a&gt; and is a fully managed service that leverages other GCP products
like Cloud SQL, GCS, Kubernetes Engine, Stackdriver, Cloud SQL and Identity Aware Proxies.&lt;/p&gt;
&lt;p&gt;You don&amp;rsquo;t have to worry about provisioning and dev-ops, so you can focus on your core business logic and let Google take care of the rest.
With Airflow it&amp;rsquo;s easy to create, schedule, and monitor pipelines that span both cloud and on-premises data centres.
With a vast selection of &lt;em&gt;&amp;ldquo;Operators&amp;rdquo;&lt;/em&gt;, you can define pipelines natively in Python with just a few lines of code,
and connect to several cloud providers or on-premises instances.
If you don&amp;rsquo;t find the operator that fits your needs, it&amp;rsquo;s incredibly easy to create your own and import it.
If you want to learn more about Apache Airflow, refer to the official &lt;a href=&#34;https://airflow.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;running-short-living-tasks&#34;&gt;Running short-living tasks&lt;/h2&gt;
&lt;p&gt;Airflow is a great tool, but as is often the case with high-level tools,
it can introduce overheads when compared to lower-level implementations of the same operation.
To get an idea of the overhead that Airflow could introduce,
we can analyse the &lt;a href=&#34;https://cloud.google.com/composer/docs/tutorials/health-check&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;health-check example&lt;/a&gt; and run it on a Cloud Composer instance.
The health-check DAG consists of a single bash operator that executes a simple command ( &lt;code&gt;echo ENVIRONMENT_NAME&lt;/code&gt;).
The following screenshot shows 5 runs of this workflow and returns run-times of 3.6 and 5 seconds.
As this scale linearly across multiple invocations, the lag becomes a real problem if - like us - you&amp;rsquo;re running thousands of them.
To validate this, let&amp;rsquo;s also measure how much time the same command run for if executed directly from the command line.&lt;/p&gt;


















&lt;figure id=&#34;figure-echo-task-information-from-airflow-ui&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./airflow_tasks_view.png&#34; data-caption=&#34;echo task information from Airflow UI&#34;&gt;


  &lt;img src=&#34;./airflow_tasks_view.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    echo task information from Airflow UI
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# time echo my-awesome-string

real    0m0.000s
user    0m0.000s
sys     0m0.000s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected &lt;code&gt;echo&lt;/code&gt; is blazing fast - and even trying to measure its execution time with the standard implementation of &lt;code&gt;time&lt;/code&gt; does not cut it.
We could recompile the kernel with &lt;code&gt;CONFIG_HIGH_RES_TIMERS=y&lt;/code&gt; - but who has time for that!
So to find out, let&amp;rsquo;s use &lt;code&gt;date&lt;/code&gt; with nanosecond precision (keep in mind that in this way the measure will be slightly overestimated).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# date +&amp;quot;%9N&amp;quot;; echo my-awesome-string ; date +&amp;quot;%9N&amp;quot;
571368370
my-awesome-string
573152179
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This output is the decimal part of the date-stamp expressed in nanoseconds, so to get the elapsed time the equation is:&lt;/p&gt;


















&lt;figure id=&#34;figure-single-task-time&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./single_task_time.png&#34; data-caption=&#34;Single Task Time&#34;&gt;


  &lt;img src=&#34;./single_task_time.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Single Task Time
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;a-more-realistic-yet-simple-pipeline&#34;&gt;A more realistic, yet simple pipeline&lt;/h2&gt;
&lt;p&gt;Now that the results are in, it is pretty clear that the actual time needed to run the &lt;code&gt;echo&lt;/code&gt; command is irrelevant
compared with the duration time of the corresponding Airflow task.
So in this case, we can say that Airflow introduces an overhead of about 4 seconds.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s apply these findings to a more real-world example. Let&amp;rsquo;s imagine we have a pipeline made of 4 tasks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run a query on BigQuery using the BigQuery operator.&lt;/li&gt;
&lt;li&gt;Export the data to GCS.&lt;/li&gt;
&lt;li&gt;Compose the exported file into a single file.&lt;/li&gt;
&lt;li&gt;Copy the composed file to S3.&lt;/li&gt;
&lt;/ol&gt;


















&lt;figure id=&#34;figure-sample-pipeline&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./pipeline_diagram.png&#34; data-caption=&#34;Sample Pipeline&#34;&gt;


  &lt;img src=&#34;./pipeline_diagram.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample Pipeline
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In our world, this pipeline runs daily for each country (of which we cover almost 250), which results in 1,000 tasks per daily run.
It takes just a few minutes to execute this operation via a simple bash script because each operation runs for just a few seconds.
Using Airflow however, this pipeline takes about an hour to complete.
When running a backfill job over 3 years of data across 3 different instances of this pipeline,
we end up with more than 3 millions tasks to execute, and having so many tasks to manage at the same time creates an avalanche effect.
This overwhelms the system and causes problems in getting tasks to schedule, queue, and run.&lt;/p&gt;
&lt;p&gt;To achieve our aim while invoking fewer tasks, we accepted a reduction in the granularity of execution and parallelism
but maintained the automation capabilities offered by Airflow.
We were able to tune this developing an Airflow plugin called Chains, which can be found in the &lt;a href=&#34;https://github.com/huq-industries/airflow-plugins&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;airflow-plugins&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;NB. If you have built a custom operator or plugins that you want to share, you can do that by submitting a PR to that repository.&lt;/p&gt;
&lt;h1 id=&#34;the-chain-airflow-plugin&#34;&gt;The Chain Airflow plugin&lt;/h1&gt;
&lt;p&gt;The Chain plugin offers 2 operators that you can use to chain together a set of operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;BigQueryChainOperator&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BigQueryToCloudStorageChainOperator&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These operators are based on the corresponding official ones ( &lt;em&gt;BigQueryOperator&lt;/em&gt; and &lt;em&gt;BigQueryToCloudStorageOperator&lt;/em&gt;) with just a few simple modifications.
They can take lists of values instead of single values as parameters, and will then loop on the list to run all the operation in sequence.&lt;/p&gt;
&lt;p&gt;This solution trades parallelism and granularity in favour of reducing the number of tasks bundling together multiple operations into a single Airflow task.
Using these operators in our example pipeline, we were able to bundle the countries
into groups and adjust the trade-off between parallelism and the total number of Airflow tasks.
In our case, by using Chains and partitioning our countries into 7 groups,
we were able to reduce the number of Airflow tasks by a factor of 35.&lt;/p&gt;
&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;The results include better performances in both the single-day and full historical backfill use-cases.
Single days run in less than 12 minutes vs.
60 minutes &lt;em&gt;(a 5x improvement)&lt;/em&gt;.
Running the 3-year backfill for our 3 different types of pipeline finish in less than a day instead of an estimated 3 months &lt;em&gt;(90x improvement)&lt;/em&gt;!
This helps to confirm that the problem was indeed the Airflow per-task overhead.&lt;/p&gt;
&lt;p&gt;Of course, this is somewhat expected, as Airflow is not designed to be a real-time system and is really designed for managing larger and more complex pipelines.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
