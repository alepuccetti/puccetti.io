[{"authors":["admin"],"categories":null,"content":"My name is Alessandro Puccetti, I am Italian üáÆüáπ but I am in fact a citizen of the world üåé. I love travelling and meeting new people from different cultures, and I enjoy having a particular focus on their food üòâ.\nü§ñ‚õ∑Ô∏èüßó‚Äç‚ôÇÔ∏è‚úàÔ∏èüçïüçùü•©ü•ì\n","date":1623715200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623715200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"My name is Alessandro Puccetti, I am Italian üáÆüáπ but I am in fact a citizen of the world üåé. I love travelling and meeting new people from different cultures, and I enjoy having a particular focus on their food üòâ.","tags":null,"title":"","type":"authors"},{"authors":[""],"categories":["Data Engineering"],"content":"   At Huq Industries, we collect and process 30 billions geo-events a month from more than 8 million devices. At the time of writing we collected and enriched more than 300 billions events all stored in BigQuery table for a total of about 85 TB.\nIn this blog post, I will demonstrate how we reduced the cost a single expensive job from more than 425$ using on-demand pricing (5$/TB) to just 25$ using flat-rate pricing.\nUsually, flat-rate pricing is generally used when you want predictable costs and have a stable and continue slots usage. However, flat-rate pricing can be leveraged also to reduce the cost of even a single job alone.\nIn this post, I will use a real-world scenario from one of our clients to show how flat-rate pricing helped us saving lots of money when performing this very data-intensive job.\nFor a more detailed explanation of what are and how BigQuery slots work refer to the official docs. To put it simply, you can think to a BigQuery slot as:\n A virtual CPU used by BigQuery to execute SQL queries.\n BigQuery Slots Commitment and Reservation Before we start using BigQuery flat-rate, we need to have a quick introduction on how to provision and allocate slots reservations. If you already have familiarity with this slots commitment and reservation feel free to skip this section and jump directly to the next part.\nWe will follow the Google best practice to set up the environment to use slots reservations. First, we create a new GCP project to only manage slots and reservation. Then, using the \u0026ldquo;Reservation\u0026rdquo; menu from te BigQuery UI, we buy a Flex slots commitment. This type of commitment is more expensive than monthly or annual ones, but it can be cancelled any time after 60 seconds. Tip: Use the right-hand side widget to get an estimation of hourly/monthly cost of your commitment.\nNow that we have available slots, we need to create a reservation. You can grant to a reservation as many slots you want (up to the number you bought) and you can create as many reservations you like. Slots not assigned to any reservation go in the default one. In case they are idle, they can be used by any reservation.\nThe last step is to create an assignment by assigning a reservation to one of our project/folder. Now we are ready to run our job using flat-rate pricing instead of on-demand.\nThe Scenario One of our client wants to run analytics on our entire events history, however, they have some specific matching criteria for qualifying an event. Therefore, the actual data they need is much smaller than our full history table. Due to the custom nature of the filtering logic, we had no other option than running a query to select all the qualify events. Below, you can find a mock query that could be used to generate the client dataset.\n Start Small For simplicity, I will calculate the cost by multiplying the reservation cost per minute and the query duration in minutes. This makes the estimations slightly lower than the actual cost because slots are paid also when they are idle. However, for our purpose the difference is negligible.\nFirstly, to test the feasibility of this approach, I run some experiments with a smaller dataset (just a month of data ~6.5 TB).\nThe table below shows durations and costs for the job executed using on-demand and using different amount of slots.\n    Duration (min) Cost($) Improvement Factor     On-Demand 2:26 32.5 -         500 Slots 5:18 1.6 20x   1,000 Slots 3:05 2 16x   2,000 Slots 1:43 2.7 12x   4,000 Slots 1:06 2.9 11x    As you can see from the results, using flat-rate pricing reduced the costs significantly. In fact, the fewer slots we use, the cheaper the job becomes. On the down side, it needs more time to complete, but this is expected. The interesting results is that the 2,000 and 4,000 slots tests showed not only a similar decrease in cost, but also and improvement in performance. This looks like a win-win situation!!! We need to remember that this analysis is valid for our non complex query and it might yield very different results for more complex jobs and workloads. I will present different scenarios and their analysis in the following blog posts.\n Let\u0026rsquo;s go a bit more in the details of the experiments results.\nI used bqvisualiser to visualize the job progress and estimation of slots usage during execution. The top part of the graph shows the units of work that BigQuery needs to perform to complete the query. The bottom part of the graph shows the estimated used slots at a certain time during the query execution.\n  On-Demand   When using on-demand pricing our slots usage profile shows that it cruises around 2,000 slots, but sometimes it bursts to 4,000 slots (however, it often uses fewer slots). This happens because when using on-demand pricing, the available slots are (loosely) \u0026ldquo;capped\u0026rdquo; to 2,000 slots per project. However, projects might access to more slots in burst from time to time.\nThe slots usage profile radically change when running the query under a reservation. It is clear that the query fully utilizes the available slots for almost 100% of the execution time. This is particularly true for the 500 slots test:\n  500 Slots   It is clear that this query uses all the available slots except for 3 parts: the first 2 dips and the tail. The first 2 dips are linked to a repartition and a consequent re-aggregation stages. Meanwhile, the tail is due to some workers needing more time to complete their writes.\n   1,000 Slots     2,000 Slots     4,000 Slots   In general, more slots make the job faster (as long as BigQuery can divide the job in enough work units). However, you can see that the job parts that do not fully utilise a small slots reservation will not use more slots even when the reservation is larger.\nMoreover, the final cost is consistently reduced, even if we end up paying for idling slots.\nGo Big Now let\u0026rsquo;s see how the full job performed. This time, I will not run the job using on-demand pricing for obvious reasons üí∏üí∏üí∏üí∏üí∏.\n    Duration (min) Cost($) Improvement Factor     On-Demand \u0026ndash; 425 -         500 Slots 75 25 17x   1,000 Slots 40 25 17x   2,000 Slots 25 33 13x   4,000 Slots 14 37 11x       500 Slots     1,000 Slots     2,000 Slots     4,000 Slots   We can see the same pattern in the estimated slots usage graph. In fact, you can see the same the 2 dips and the tail.\nIn our case, the query has only a few and short dips and tail. So, we could continue improving performance by simply buying more slots. You can see that when we test 4,000 slots the job complete faster than with 2,000, but it costs more. This is because the time we save adding more slots is not enough to compensate the idle slots time.\nConclusion In conclusion, flat-rate pricing proved to be enormously beneficial to reduce the cost of our data-intensive job. However, beforehand it can be tricky to decide when to use flat-rate or on-demand pricing for a job. Intuitively, we can say that a good candidate for flat-rate pricing is a job that:\n Will scan lots of data (it will cost lots of money using on-demand pricing). Has a very consistent usage of slots during its execution time. The job doesn\u0026rsquo;t have strict time completion requirements.  Stay tuned for more in depth experiments and analysis on BigQuery slots and different workloads evaluation.\nFollow or contact me on Twitter for more stories about data, infrastructure, and automation.\n","date":1623715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623715200,"objectID":"50098f93452296fa929738634401c55a","permalink":"https://alepuccetti.github.io/post/slots-to-the-wallet-rescue/","publishdate":"2021-06-15T00:00:00Z","relpermalink":"/post/slots-to-the-wallet-rescue/","section":"post","summary":"At Huq Industries, we collect and process 30 billions geo-events a month from more than 8 million devices. At the time of writing we collected and enriched more than 300 billions events all stored in BigQuery table for a total of about 85 TB.","tags":["BigQuery"],"title":"Slots to the (Wallet) Rescue","type":"post"},{"authors":[""],"categories":["Data Engineering","Automation"],"content":"   In one of my previous blog posts, we have seen how, at Huq Industries, we used authorised views to reduce costs, complexity, and delivery time. You can read more about it here. In this post, we will see how to implement authorised views in production and managing them as code. This solution enables us to easily manage 100s of clients, each one with unique data requirements. To better manage and automate our workflow, we source control and review all our Google Cloud Platform data infrastructure using GitHub and Terraform Cloud. This enabled us to easily communicate, track, and maintain infrastructure changes. Let‚Äôs see how to define views and authorize them in the needed datasets. First, we would need a dataset in the client project to contain a view, then we would need the view itself, and finally the authorization.\nDefine the dataset  Define the view  Lastly, we have to authorise the view to access any needed dataset (in this example just one)  After having generated and checked the plan you can apply it. The view will then be created so that each user who has access to it can run queries without having direct access to the underlying tables.\nTo easily scale this to 100s of clients, we can write a terraform module that implements the last 2 steps and invoke it for each client we need to provision with just different variables such as: client_project_id, client_dataset_name, view_name, view_query, and the list of project_id, datase_id, to autorise.\nOne of the nicest things of authorised views is that the authorisation works as a chain. At Huq, we leverage this behaviour so that we just have to authorise the clients views to the dataset that directly accesses them. We use authorised views almost everywhere so that if one of these views are used by some other views, the end user does not need extra authorisation.\nWARNING If you also manage datasets access via other terraform resources such as: bigquery_dataset_iam_policy, bigquery_dataset_iam_binding, or bigquery_dataset_iam_member. They will fight over what the policy should be with google_bigquery_dataset_access. In fact, using any of the google_bigquery_dataset_* resources will result in removing any authorised views from that dataset previously configured via google_bigquery_dataset_access.\nYou can read more about this in the terraform docs.\nFollow or contact me on Twitter for more stories about data, infrastructure, and automation.\n","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"5389342df1eac00ad6ecff642cd587f3","permalink":"https://alepuccetti.github.io/post/bigquery-authorized-views-with-terraform/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/post/bigquery-authorized-views-with-terraform/","section":"post","summary":"Define and manage BigQuery authorized views in Terraform.","tags":["BigQuery"],"title":"BigQuery Authorized Views with Terraform","type":"post"},{"authors":[""],"categories":["Data Engineering","Automation"],"content":"   At Huq Industries, we collect and process 30 billions events a month from more than 8 million devices. Every day, we enrich, slice, and then deliver data feeds in various forms to our clients mainly via BigQuery, GCS, or S3. In the past 2 years, we have seen our data ingestion growing 4x each year and we forecast the same for the next years to come. Our already large data history and its steady and high grow rate pose several challenges from ingestion to storage, from processing to delivery and others. All these parts can be very critical. However, in this specific post, we will focus on the processing and delivery. We will see how we leveraged BigQuery Authorized Views to cut our storage and processing costs, to reduce our delivery time to zero and to streamline the whole system by removing many processes and dependencies.\nThe issue Many of our clients access our data directly using our self-served analytics offering via BigQuery sandboxed instances. At the very beginning, our system was creating and updating tables for each individual client in its own instances. This choice was based on how the core processing system created and updated the master tables (one table per country updated daily). This soon resulted in a lot of duplicated data which, in turn, was increasing our costs with the data growth and the number of clients. Hence, we needed to find a different design before it would have become unsustainable.\nThe solution: Authorized Views Everywhere Now that we leveraged BigQuery clustering (see previous blogpost), instead of sharding data into different tables for each country, we were able to simplify part of the data processing and delivery system removing ETL jobs and dependencies. This resulted in cost reduction for both processing and storage. Last, but definitely not least, it removed the delay in delivery time from when the data is ready to when the data is available to the clients.\nWhat is an authorised view?\nAn authorised view is a SQL view that was authorised to read tables in a specific dataset. This means that users, who are allowed to access the view, do not need to get permission to access the underlined tables. It is the view itself that holds that permission and acts as a sort of middle-man.\nHow is this useful for us?\nIn our case, we were able to provide all our clients with a view that grants them access only to the data they are subscribing to. Each view can be different and customized according to each client\u0026rsquo;s needs but all of them use the same source tables. Thus, removing a lot of duplicated data and updating processes.\n Now, let‚Äôs assume that we have 10s of clients who subscribed to different timeframes and/or countries to our data. Additionally, some of them needed customization of the table schema. As you can imagine, the ETL jobs and their scheduling become quite complicated and pricey. Clearly, this won‚Äôt be sustainable when scaling up to 100s or 1000s of clients. Authorised views enabled us to optimize this issue and we can now easily serve any number of clients with custom data requirements with very little overhead. This can be done by simply creating a view and authorising it to the master dataset:\n After we created this view and we granted users access to it, they still won‚Äôt be able to successfully run their queries. In fact, if a user does not have access to the underlying tables (i.e. table_a and table_b) and he/she runs a query using this view, that will result in a permission error. However, by authorizing the view to access dataset_a and dataset_b that will enable users to successfully run their queries. This can easily be done in the BigQuery explorer. To get to the ‚Äúauthorised views‚Äù menu go to source_project_id ‚ûù dataset_a ‚ûù \u0026ldquo;Share Dataset\u0026rdquo; ‚ûù \u0026ldquo;Authorised Views\u0026rdquo;. Now, you can configure which views have access to this dataset using their project_id, dataset_id, and view_id. In our example: client_project, dataset, and view_name. Repeat this for all the dataset that the view needs to access, in our example we have 2 datasets in 2 different projects.\n Bonus: views using views BigQuery, before running a query, resolves all the views and finds all the source tables and verifies that the user or, in our case, the view can read from all them. Otherwise will return a permission error. What we saw so far works well if our views use tables. What if our authorised view uses another view? Would this solution still work as is? Does our view need to be authorised on all the dataset used by all the used views and their views etc.?\nThe answer is no, at least not directly.\nOf course you could authorise the final view to access all of these tables but, as you can imagine, this will result in a lot of dependencies, repetitions, and complex definitions that will be a nightmare to maintain.\nHow can we successfully use views in an authorised view?\nThe answer is authorised views. More precisely, we can use authorised views in another authorised view because the ‚Äúinner‚Äù view will be authorised to access the resources creating a chain of authorisation. In this way, we can just simply manage the authorisation of each single view.\nStay tuned for an implementation of this using Terraform in the coming blogposts.\n Follow or contact me on Twitter for more stories about data, infrastructure, and automation.\n","date":1617148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617148800,"objectID":"31fa6d0198ee499cd2a0ffa4ae8c2cd5","permalink":"https://alepuccetti.github.io/post/bigquery-real-time-data-delivery-at-scale/","publishdate":"2021-03-31T00:00:00Z","relpermalink":"/post/bigquery-real-time-data-delivery-at-scale/","section":"post","summary":"Leveraging BigQuery authorized views for fun and profit to deliver real-time data delivery as scale.","tags":["BigQuery"],"title":"Real-Time data delivery at scale with BigQuery","type":"post"},{"authors":[""],"categories":["Data Engineering"],"content":"   Who does not like time travel? We all saw it and were fascinated by it in many sci-fi movies, unfortunately science did not crack real-life time travel yet.\nHowever, we can ‚Äúdata time-travel‚Äù. Thanks to the amazing BigQuery SQL feature ‚ÄúFOR SYSTEM_TIME AS OF‚Äù, we can time travel (up to 7 days in the past) to a specific timestamp (for detailed information about the syntax refer to the official documentation).\nThis feature is extremely easy to use. In fact, we can simply modify any query by adding the clause above mentioned to the FROM clause. The following example returns all rows in table_a from exactly 3 hours ago.\n Or we can specify a precise point-in-time.\n How is this useful? Ok, it is nice to talk about time travel, but how is this useful in real-life? In the current age of high data integration, data is inserted, deleted, and modified at a high rate. In highly integrated and streaming systems, it is often challenging to plan and perform backups that can be used to recover from processing errors. In BigQuery this can be done by just selecting rows from a specific point-in-time. This removes the need to materialize snapshots (for the past 7 days) and give us incredible granularity (timestamp precision). The most obvious use-case is data recovery. We can easily recover a table state by creating a new one from a specific point-in-time.\n A more interesting and less obvious use-case could be to ensure consistent analytics results over time from a table that is modified with a high frequency or that receives streaming inserts. In this case, we could configure the SQL query to always return results from a specific timestamp during a specific time frame. We could build a caching system to do that or we could use ‚ÄúFOR SYSTEM_TIME AS OF‚Äù with a specific point-in-time to ‚Äúforce‚Äù queries to always return results from a past table state and just change the value when we want the results from a different time.\n Or for a more dynamic solution\n Follow or contact me on Twitter for more stories about data, infrastructure, and automation.\n","date":1615248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615248000,"objectID":"743814f1a12d4d57facff78dd38f65a6","permalink":"https://alepuccetti.github.io/post/bigquery-time-travel/","publishdate":"2021-03-09T00:00:00Z","relpermalink":"/post/bigquery-time-travel/","section":"post","summary":"How to use BigQuery superpowers to rewind time.","tags":["BigQuery"],"title":"Time Travel with BigQuery","type":"post"},{"authors":[""],"categories":["Data Engineering","Analytics"],"content":"   Multi-cloud Data Analytics   Do you use BigQuery? Are you interested in knowing how to integrate data from different cloud providers into BigQuery? In this blogpost, we will implement a serverless and fully managed system to make available S3 access logs into BigQuery to easily integrate them with other data sources and reporting systems. To achieve this we will see how to set up AWS S3 access logs delivery and configure Google Data Transfer Service in order to schedule fully managed S3 to Cloud Storage transfers.We will also use BigQuery external table to read data directly from Google Cloud Storage to access always to the most recent data without keeping reloading data into BigQuery. Finally, we will leverage persistent UDFs to parse the logs into tabular format on the fly.\nSet up S3 access logging AWS allows you to export S3 access logs, which can be used to monitor user access and build business metrics. Although, you can export logs for a specific bucket into itself, this might cause extra logs to get generated. In our scenario we use a different bucket to collect and store the access logs of many other buckets. Also, S3 access logs can be exported only to a bucket in the same region for the monitored one. So we will need to create a different bucket for each region where we have buckets that we want to monitor. In this scenario, we will work in the \u0026ldquo;eu-west-1\u0026rdquo; region. Let‚Äôs create a bucket to collect and store the access logs.\naws s3api create-bucket --bucket MY_BUCKET_EU-WEST-1 --region eu-west-1  Now we need to set up S3 access logs delivery. First we grant the AWS LogDelivery group the relevant permissions to our destination bucket:\naws s3api put-bucket-acl --bucket MY_BUCKET_EU-WEST-1 --grand-write URI=http://acs.amazonaws.com/groups/s3/LogDelivery --grant-read-acp URI=http://acs.amazonaws.com/groups/s3/LogDelivery  Then we define the logging configuration locally in a JSON file:\n{ \u0026quot;LoggingEnabled\u0026quot;: { \u0026quot;TargetBucket\u0026quot;: \u0026quot;MY_BUCKET_EU-WEST-1\u0026quot;, \u0026quot;TargetPrefix\u0026quot;: \u0026quot;BUCKET_TO_LOG/\u0026quot;, } }  TargetBucket is the bucket where the logs will be delivered. You can provide a prefix for the logs using TargetPrefix (optional), we use it to separate the logs of different buckets but you could use a different logic. Now we need to enable logging:\naws s3api put-bucket-logging --bucket BUCKET_TO_LOG --bucket-logging-status file://logging.json  It‚Äôs worthy to remember that logs are delivered on a best effort basis, usually most log records are delivered within a few hours of the time they are recorded, but can be more frequently.\nTransfer logs to Google cloud Platform In our scenario, we will transfer the data first into Cloud Storage using the Data Transfer service and then using BigQuery external table to run queries on them. Another possible solution would be to use S3 to BigQuery data transfer service to directly transfer the data into BigQuery. The Data Transfer service is fully managed, so you just need to configure it and you won‚Äôt need to provision and monitor any infrastructure. Transfers are scheduled to run every 24 hours, if you need a different interval you will have to implement the data transfer yourself instead of using this service. Before setting up the transfer we need a destination bucket on Cloud Storage. Let‚Äôs create a multi-regional bucket in Europe.\ngsutil mb -l EU gs://MY_AWS_S3_ACCESS_LOGS_MIRROR/  To set up the transfer service, log into your cloud console to find Data Transfer under the Storage section. Now click on \u0026ldquo;Create a transfer job\u0026rdquo; and follow the steps below: As a source we select \u0026ldquo;Amazon S3 bucket and include the bucket name we want to read from, in our example \u0026ldquo;MY-S3-ACCESS-LOGS-EU-WEST-1\u0026rdquo;. We need to provide AWS authentication Access key ID and Secret access key that have the permission to read the bucket. As the destination, we put the bucket we just created MY_AWS_S3_ACCESS_LOGS_MIRROR and leave all the other options un-ticked. We select to run daily at a specific time. Click on Create.\nFor more detailed information refer to the official documentation.\nWe still need to be able to access the data in BigQuery to do our analytics and build reporting dashboards with Data Studio. Unfortunately, AWS S3 log entries are not in JSON or CSV-like format but they use their own. For this reason, BigQuery cannot automatically load it into a table with the desired schema. To have the logs parsed as we desire we will use a persistent UDF but first we need to make the log entries \u0026ldquo;available\u0026rdquo; to BigQuery. To do so, we define an external table that reads data from our mirror bucket in Cloud Storage and we have to configure it as the files use newline as field separator. As a result, we want a table where each row contains an entire log entry, we choose the newline character as field separator because we do not expect to have any of them in our log entries. This will result in a table with a single column. To create our source table we can use the cloud console: Select the dataset you want to create your table in and click create table. In the table creation menu, select as source \u0026ldquo;Google Cloud Storage\u0026rdquo;, CSV as format, and add the prefix of the data that you want to be the sources of the table (remember that you can use wildcard to say to BigQuery to select all the files within the prefix). Choose your table name (in this example we will use \u0026ldquo;s3_raw_logs_external\u0026rdquo;) and because we do not want to actually load the data, we need to select \u0026ldquo;External table\u0026rdquo; in \u0026ldquo;Table type\u0026rdquo;. In the schema section add a single column called \u0026ldquo;text\u0026rdquo; of type STRING. Click on \u0026ldquo;Advanced options\u0026rdquo; and set the field delimiter to be \u0026ldquo;Custom\u0026rdquo; and insert s character that should never be present (e.g. \u0026ldquo;¬ß\u0026quot;). You can leave the other options with their default values and click \u0026ldquo;create\u0026rdquo;.\nNow we have a table into BigQuery where the source data is in Cloud Storage, this means that every query reading from this table will always acces all the data present in the bucket at that time. Note that this setup might start having degraded performances for very large logs datasets.\nTransform raw logs into tabular format As we said above , AWS S3 log entries are not in JSON or CSV-like format but they use their own format, so we need to parse them ourselves . We can easily do this directly in BigQuery using a persistent UDF. I wrote a Javascript UDF to parse most of the log entry parts into an object. Note that we use \u0026ldquo;\\\u0026rdquo; instead of \u0026ldquo;\u0026quot; because we need to escape when defining the function.\n You can copy and paste the code above into the BigQuery UI to create the s3_log_parser as a persistent UDF, just replace PROJECT_ID and DATASET_ID with valid values for your environment.\nNow we have easy access to the data and the logic to transform it into tabular format, the following query will parse all the logs entry into the column defined in the UDF.\n Let‚Äôs copy and paste this query into the BigQuery UI and run it. If you have already data into the Cloud Storage bucket you will see the logs parsed out. Now click on \u0026ldquo;Save view\u0026rdquo;, choose a destination dataset and a name for the view (e.g. s3_logs_parsed). In this way, we can use the view as a source for our analytics reporting dashboards instead of always writing the query itself. Now try the following query, it should show the same results of the previous one.\nSelect * from `PROJECT_ID.DATASET_ID.s3_logs_parsed`  Now you have a serverless, fully-managed system to analyse your AWS S3 access logs into BigQuery using SQL and play with Google Datastudio to build and share reporting dashboards.\n","date":1589068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589068800,"objectID":"057816b162cd161b678d290ed7c0d696","permalink":"https://alepuccetti.github.io/post/serverless-s3-access-logs-analytics-with-bigquery/","publishdate":"2020-05-10T00:00:00Z","relpermalink":"/post/serverless-s3-access-logs-analytics-with-bigquery/","section":"post","summary":"Configuring a Serverless and full-managed system to analyze S3 access logs using BigQuery.","tags":["BigQuery"],"title":"Serverless S3 Access Logs Analytics using BigQuery","type":"post"},{"authors":[""],"categories":["Data Engineering","Geospatial"],"content":"   World Map   The BigQuery team rolled out support for geography type a while ago and they have never stopped improving performances and Geographic Information System functions (GIS). This allows users to run complex geo-spatial analytics directly in BigQuery harnessing all its power, simplicity, and reliability.\nHold on your keyboard (or your screen if you are reading this on a mobile device).\nNow you can cluster tables using a geography column. Say what!!!!\nThis is game changing for users working heavily with geodata. By clustering your table on a geography column, BigQuery can reduce the amount of data that needs to read to serve the query. This makes queries cheaper and run faster when filtering on clustering column.\nLet\u0026rsquo;s see the benefits of clustering table using geography column with an example. We will use one of the great public datasets curated by Felipe Hoffa. We will use the weather_gsod specifically the two tables all and all_geoclustered.\nLet\u0026rsquo;s say that we want the all-time minimum and maximum temperature within the Greater London area for each station. Our query will look like this:\n Results:\nname min_temp max_temp ST JAMES PARK 26.3 83.2 KENLEY AIRFIELD 18.0 82.8 HEATHROW 18.6 83.4 CITY 22.3 92.2 BLACKWALL 37.0 62.2 NORTHOLT 18.4 84.1 BIGGIN HILL 15.5 90.3 PURLEY OAKS 23.0 81.7 LEAVESDEN 22.3 89.1 LONDON WEA CENTER 20.0 85.2 KEW-IN-LONDON 23.3 82.4  This query reads 9.02GB of data. Now let\u0026rsquo;s see how the same query perform on the clustered table:\n The result is obviously the same but, this time, BigQuery reads just 98.97MB of the data!!! So switching to the geo clustered table made this query almost 100 times cheaper. Do you want to try other locations and test the differences yourself? You can use Huq Industries GisMap tool to quickly draw and export polygons in various formats.\n  Huq GisMap   ","date":1580342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580342400,"objectID":"c83af019d3e93e2045acb557e86a744e","permalink":"https://alepuccetti.github.io/post/bigquery-geography-clustering/","publishdate":"2020-01-30T00:00:00Z","relpermalink":"/post/bigquery-geography-clustering/","section":"post","summary":"Improve your geospatial analytics performance in minutes.","tags":["BigQuery"],"title":"BigQuery Geography Clustering","type":"post"},{"authors":[""],"categories":["Data Engineering"],"content":"   Shipping Containers   In this blogpost, I will explain what partitioning and clustering features in BigQuery are and how to supercharge your query performance and reduce query costs.\nPartitioning Partitioning a table can make your queries run faster while spending less. Until December 2019, BigQuery supported table partitioning only using date data type. Now, you can do it on integer ranges too. If you want to know more about partitioning your tables this way, check out this great blogpost by Guillaume Blaquiere.\nHere, I will focus on date type partitioning. You can partition your data using 2 main strategies: on the one hand you can use a table column, and on the other, you can use the data time of ingestion.\nThis approach is particularly useful when you have very large datasets that go back in time for many years. In fact, if you want to run analytics only for specific time periods, partitioning your table by time allows BigQuery to read and process only the rows of that particular time span. Thus, your queries will run faster and, because they are reading less data, they will also cost less.\nCreating a partitioned table is an easy task. At the time of table creation, you can specify which column is going to be used for partitioning, otherwise, you can set up the partitioning on ingestion time. Since you can query this table in the same exact way of those that are not partitioned, you won\u0026rsquo;t have to change a line of your existing queries.\n Assuming that \u0026ldquo;sampling_date\u0026rdquo; is the partitioning column, now BigQuery can use the specified values in the \u0026ldquo;where clause\u0026rdquo; to read only data that belong to the right partitions.\nBonus nugget You can use partition decorators to update, delete, and overwrite entire single partitions as in:\n# overwrite single partition loading from file bq load ‚Äî-replace \\ project_id:dataset_name.table_name$20190805 \\ gs://my_input_bucket/data/from/20190805/* ./schema.json  And\n# overwrite single partition from query results bq query ‚Äî- replace --use_legacy_sql=false \\ ‚Äî-destination_table project_id:dataset.table$20190805 \\ 'select * from project_id:dataset.another_table'  In the cases above, both the loaded data and the query results have to belong to the referenced partition, otherwise the job will fail.\nClustering Clustering is another way of organizing data which stores one next to the other all those rows that share similar values in the chosen clustering columns. This process increases the query efficiency and performances. Note that BigQuery supports this feature only on partitioned tables.\nBigQuery can leverage clustered tables to read only data relevant to the query, so it becomes faster and cheaper.\nAt the table creation time, you can provide up to 4 clustering columns in a comma-separated list e.g. \u0026ldquo;wiki\u0026rdquo;, \u0026ldquo;title\u0026rdquo;. You should also keep in mind that their order is of paramount importance but we will see this in a moment.\nIn this section we will use \u0026ldquo;wikipedia_v3\u0026rdquo; form Felipe Hoffa\u0026rsquo;s public dataset, which contains yearly tables of Wikipedia page views. These are partitioned by the \u0026ldquo;datehour\u0026rdquo; column and clustered on \u0026ldquo;wiki\u0026rdquo; and \u0026ldquo;title\u0026rdquo; columns. A single row may look like this:\ndatehour, language, title, views 2019‚Äì08‚Äì10 03:00:00 UTC, en, Pizza, 106 ...  The following query counts, broken-down per year, all the page views for the Italian wiki from 2015‚Äì01‚Äì01.\n If you write this query in BigQuery UI, it will estimate a data scanning of 4.5 TB. However, if you actually run it, the final scanned data will be of just 160 GB.\nHow is this possible?\nWhen BigQuery reads only read rows belonging to the cluster that contains the data for the Italian wiki while discarding everything else.\nWhy is the columns order so important in clustering?\nIt is important because BigQuery will organize the data hierarchically according to the column order that is specified when the table is created.\nLet\u0026rsquo;s use the following example:\n This query needs to access all the \u0026ldquo;wiki\u0026rdquo; clusters and then it can use the \u0026ldquo;title\u0026rdquo; value to skip the not matching clusters. This results in scanning a lot more data than if the clustering columns were in the opposite order \u0026ldquo;title\u0026rdquo;, \u0026ldquo;wiki\u0026rdquo;. At the time of writing, the query above estimated a scanning cost of 1.4 TB but it actually scanned only 875.6 GB of data. Let\u0026rsquo;s now invert the clustering columns order putting first \u0026ldquo;title\u0026rdquo; and second \u0026ldquo;wiki\u0026rdquo;, you can do so using the following command:\nbq query --allow_large_results --nouse_legacy_sql \\ --destination_table my_project_id:dataset_us.wikipedia_2019 \\ --time_partitioning_field datehour \\ --clustering_fields=title,wiki \\ 'select * from `fh-bigquery.wikipedia_v3.pageviews_2019`'  Running the \u0026ldquo;Pizza\u0026rdquo; query on our new table \u0026ldquo;my_project_id:dataset_us.wikipedia_2019\u0026rdquo; should be much cheaper. In fact, while the estimation was still of 1.4 TB, the actual data read was just of 26.3 GB, that is 33 times less.\nAs final test let\u0026rsquo;s try filtering on the \u0026ldquo;wiki\u0026rdquo; column:\n The data read estimation is always the same but now the actually data read jumped to 1.4 TB (the entire table) whereas, in the first example, the actually data read was just of 160 GB.\nNote: Since BigQuery uses a columnar store, \u0026ldquo;title is not null\u0026rdquo; ensures that we refer always to the same number of columns in every query. Otherwise, the data read from the last query is lower because we refer to fewer columns.\nIt is evident that choosing the right clustering columns and their order makes a great difference. You should plan it accordingly to your workloads.\nRemember, always partition and cluster your tables! It is free, it does not need to change any of your queries and it will make them cheaper and faster.\n","date":1579564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579564800,"objectID":"acd4006cf189eb25aa667a23dcd3b0d2","permalink":"https://alepuccetti.github.io/post/bigquery-partitioning-clustering/","publishdate":"2020-01-21T00:00:00Z","relpermalink":"/post/bigquery-partitioning-clustering/","section":"post","summary":"Improving performance at a fraction of the cost.","tags":["BigQuery"],"title":"BigQuery Partitioning \u0026 Clustering","type":"post"},{"authors":[""],"categories":["Data Engineering"],"content":"BigQuery supports the \u0026ldquo;*\u0026quot; wildcard to reference multiple tables or files. You can leverage this feature to load, extract, and query data across multiple sources, destinations, and tables. Let\u0026rsquo;s see what you can do with wildcards with some examples.\nThe first thing is definitely loading the data into BigQuery. If you deal with a very large amount of data you will have, most likely, tens of thousands of files coming from a data pipelines that you want to load into BigQuery. Using wildcards, you can easily load data from different files into a single table.\nbq load project_id:dataset_name.table_name gs://my_data/input/prefix/* ./schema.json  Also, this is not limited to only one prefix but you can specify multiple ones for example:\nbq load project_id:dataset_name.table_name gs://my_data/input/prefix_1/* gs://my_data/input/prefix_5/* gs://my_data/input/prefix_25/* ./schema.json  The command above will load all the files matching all the prefixes into the specified table.\nWildcards can be used in the other direction too. Namely, they can be used to export data from BigQuery to GCS. This is very useful especially because BigQuery limits exports to a single file only to tables smaller than 1GB.\nbq extract project_id:dataset_name.table_name gs://my_data/extract/prefix/file_prefix_*.json  The previous command will result in multiple files exported into the \u0026ldquo;my_data\u0026rdquo; bucket within the prefix \u0026ldquo;extract/prefix/\u0026quot; and all file names will be:\nfile_prefix_000000000000.json file_prefix_000000000001.json file_prefix_000000000002.json ‚Ä¶ file_prefix_000000003792.json file_prefix_000000003793.json  The other very useful use of wildcards is evident in queries. In fact, you can reference multiple tables in a single query by using \u0026quot;\u0026quot;* to match all the table into the dataset with the same prefix. For example, consider you have a collection of tables like:\nmy_dataset ‚îú‚îÄ‚îÄ events_GB ... ‚îú‚îÄ‚îÄ events_IT ... ‚îú‚îÄ‚îÄ events_US ...  The following query will return the count per day per country of events of type \u0026ldquo;submit\u0026rdquo; in our dataset.\n You can also filter out matched tables using \u0026quot;_table_suffix\u0026rdquo; in the where clause. For example, if you are only interested in Germany, France, and Japan just run the following:\n What I personally like the most of using wildcards is that it enables me to design better, simpler, and more generic analytics queries as well as ETL jobs. The aim of this post was to help you improve your code quality and your productivity.\nIf you believe you learnt something new or if you liked the post please Clap. Happy query with BigQuery!!!\n","date":1578268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578268800,"objectID":"1bde0b59c82d5584a24c342a96224edc","permalink":"https://alepuccetti.github.io/post/bigquery-wildcards/","publishdate":"2020-01-06T00:00:00Z","relpermalink":"/post/bigquery-wildcards/","section":"post","summary":"Managing massive datasets using wildcards.","tags":["BigQuery"],"title":"BigQuery Wildcards","type":"post"},{"authors":[""],"categories":["Data Engineering"],"content":"   BigQuery   Back in the early days of Huq we were ingesting a just few millions records per day into our geo-behavioural insights platform. Today that figure is in the hundreds of millions. During the period where our traffic was ramping intensively, we quickly realised that our single high-spec bare metal server setup was not going to be enough for our analytics needs.\nAfter all, what good is building a valuable data asset if you can\u0026rsquo;t get answers out? We wanted to find a way to retrieve answers in seconds, not days, and so we set ourselves a mission: find a solution that allows us to query and obtain answers to sometimes complex, often spatial, queries across billions of records in minutes at most. We wanted great performance, high reliability, to make use of our SQL skills. And all that with minimum set-up, and minimal DevOps management overhead. Moon on a stick? Definitely.\nWe tested different solutions but it quickly became clear that BigQuery was a step (often several steps) ahead of the myriad other solutions that we evaluated. We found BigQuery to be: effortless to set up and use with virtually zero management and DevOps overhead. It has with blazingly fast performance, flexibility and portability. We can\u0026rsquo;t remember the last time we had to carve out a subset of our data, build indices and otherwise optimise our queries. Load your data, buckle up and go.\nBecause it is fully managed you get a lot of great features out-of-the-box: high availability, serverless computing, automatic backups, logging and auditing.\nMoreover, BigQuery UI is a great tool that allowed the team to get up and running fast. BigQuery\u0026rsquo;s Standard SQL removes the need to learn a new language, or set up special DB client (although you can use many different SQL clients to connect to BigQuery). It also offers a rapidly expanding menu of GIS functions that in some ways are easier to work with than those found in PostGIS. Last but not least, BigQuery offers a simple pricing model that makes it very easy to estimate cost for internal and external projects, and to keep a firm handle on spend.\nGoogle BigQuery has dramatically improved our access to analytics and the insights that arise from our dataset. Even relatively simple queries that in our heavily-optimised bare-metal environment still ran for several hours, on BigQuery returns seconds. BigQuery has helped us to improve the breadth and quality of our analytical offerings through speed, complexity and the ability to iterate. We also now serve many of our clients using BigQuery ‚Äî either alone or as a backend. BigQuery makes it possible offer direct access to our datasets to in-house data science teams, or to power specialised dashboards according to customer needs.\nFor us, Google BigQuery has helped our business make a huge leap forward ‚Äî and there\u0026rsquo;s no going back from here. Stay tuned for more technical follow-ups on our use of Google BigQuery.\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"d0f69e419f59259ff37fd49c665daf9f","permalink":"https://alepuccetti.github.io/post/bigquery-jounrey/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/bigquery-jounrey/","section":"post","summary":"BigQuery   Back in the early days of Huq we were ingesting a just few millions records per day into our geo-behavioural insights platform. Today that figure is in the hundreds of millions.","tags":["BigQuery"],"title":"A Journey Through Big Data Using Google BigQuery","type":"post"},{"authors":[""],"categories":["Data Engineering"],"content":"   Google Cloud Composer   Introduction Google Cloud Composer is Google Cloud Platform product that helps you manage complex workflows with ease.\nIt is built on top of Apache Airflow and is a fully managed service that leverages other GCP products like Cloud SQL, GCS, Kubernetes Engine, Stackdriver, Cloud SQL and Identity Aware Proxies.\nYou don\u0026rsquo;t have to worry about provisioning and dev-ops, so you can focus on your core business logic and let Google take care of the rest. With Airflow it\u0026rsquo;s easy to create, schedule, and monitor pipelines that span both cloud and on-premises data centres. With a vast selection of \u0026ldquo;Operators\u0026rdquo;, you can define pipelines natively in Python with just a few lines of code, and connect to several cloud providers or on-premises instances. If you don\u0026rsquo;t find the operator that fits your needs, it\u0026rsquo;s incredibly easy to create your own and import it. If you want to learn more about Apache Airflow, refer to the official docs.\nRunning short-living tasks Airflow is a great tool, but as is often the case with high-level tools, it can introduce overheads when compared to lower-level implementations of the same operation. To get an idea of the overhead that Airflow could introduce, we can analyse the health-check example and run it on a Cloud Composer instance. The health-check DAG consists of a single bash operator that executes a simple command ( echo ENVIRONMENT_NAME). The following screenshot shows 5 runs of this workflow and returns run-times of 3.6 and 5 seconds. As this scale linearly across multiple invocations, the lag becomes a real problem if - like us - you\u0026rsquo;re running thousands of them. To validate this, let\u0026rsquo;s also measure how much time the same command run for if executed directly from the command line.\n  echo task information from Airflow UI   # time echo my-awesome-string real 0m0.000s user 0m0.000s sys 0m0.000s  As expected echo is blazing fast - and even trying to measure its execution time with the standard implementation of time does not cut it. We could recompile the kernel with CONFIG_HIGH_RES_TIMERS=y - but who has time for that! So to find out, let\u0026rsquo;s use date with nanosecond precision (keep in mind that in this way the measure will be slightly overestimated).\n# date +\u0026quot;%9N\u0026quot;; echo my-awesome-string ; date +\u0026quot;%9N\u0026quot; 571368370 my-awesome-string 573152179  This output is the decimal part of the date-stamp expressed in nanoseconds, so to get the elapsed time the equation is:\n  Single Task Time   A more realistic, yet simple pipeline Now that the results are in, it is pretty clear that the actual time needed to run the echo command is irrelevant compared with the duration time of the corresponding Airflow task. So in this case, we can say that Airflow introduces an overhead of about 4 seconds.\nLet\u0026rsquo;s apply these findings to a more real-world example. Let\u0026rsquo;s imagine we have a pipeline made of 4 tasks:\n Run a query on BigQuery using the BigQuery operator. Export the data to GCS. Compose the exported file into a single file. Copy the composed file to S3.    Sample Pipeline   In our world, this pipeline runs daily for each country (of which we cover almost 250), which results in 1,000 tasks per daily run. It takes just a few minutes to execute this operation via a simple bash script because each operation runs for just a few seconds. Using Airflow however, this pipeline takes about an hour to complete. When running a backfill job over 3 years of data across 3 different instances of this pipeline, we end up with more than 3 millions tasks to execute, and having so many tasks to manage at the same time creates an avalanche effect. This overwhelms the system and causes problems in getting tasks to schedule, queue, and run.\nTo achieve our aim while invoking fewer tasks, we accepted a reduction in the granularity of execution and parallelism but maintained the automation capabilities offered by Airflow. We were able to tune this developing an Airflow plugin called Chains, which can be found in the airflow-plugins repository.\nNB. If you have built a custom operator or plugins that you want to share, you can do that by submitting a PR to that repository.\nThe Chain Airflow plugin The Chain plugin offers 2 operators that you can use to chain together a set of operations:\n BigQueryChainOperator BigQueryToCloudStorageChainOperator  These operators are based on the corresponding official ones ( BigQueryOperator and BigQueryToCloudStorageOperator) with just a few simple modifications. They can take lists of values instead of single values as parameters, and will then loop on the list to run all the operation in sequence.\nThis solution trades parallelism and granularity in favour of reducing the number of tasks bundling together multiple operations into a single Airflow task. Using these operators in our example pipeline, we were able to bundle the countries into groups and adjust the trade-off between parallelism and the total number of Airflow tasks. In our case, by using Chains and partitioning our countries into 7 groups, we were able to reduce the number of Airflow tasks by a factor of 35.\nConclusions The results include better performances in both the single-day and full historical backfill use-cases. Single days run in less than 12 minutes vs. 60 minutes (a 5x improvement). Running the 3-year backfill for our 3 different types of pipeline finish in less than a day instead of an estimated 3 months (90x improvement)! This helps to confirm that the problem was indeed the Airflow per-task overhead.\nOf course, this is somewhat expected, as Airflow is not designed to be a real-time system and is really designed for managing larger and more complex pipelines.\n","date":1563148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563148800,"objectID":"488ff75e193273f578f94a29128fb73a","permalink":"https://alepuccetti.github.io/post/composer-short-living-tasks/","publishdate":"2019-07-15T00:00:00Z","relpermalink":"/post/composer-short-living-tasks/","section":"post","summary":"Improving efficient in executing millions of short living tasks.","tags":["Automation"],"title":"Google Cloud Composer: Overcoming The Short-living Tasks Problem","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://alepuccetti.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]